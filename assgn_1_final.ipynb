{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning  Assignment\n",
    "**by- ed24s401**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER 1\n",
    "\n",
    "Download the fashion-MNIST dataset and plot 1 sample image for each class as shown in the grid below. Use from keras.datasets import fashion_mnist for getting the fashion mnist dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Control-C detected -- Run data was not synced\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m#logging figure to wandb\u001b[39;00m\n\u001b[32m     19\u001b[39m wandb.log({\u001b[33m\"\u001b[39m\u001b[33mfashion-MNIST samples\u001b[39m\u001b[33m\"\u001b[39m: marked_img})\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mwandb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data science\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:4130\u001b[39m, in \u001b[36mfinish\u001b[39m\u001b[34m(exit_code, quiet)\u001b[39m\n\u001b[32m   4113\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Finish a run and upload any remaining data.\u001b[39;00m\n\u001b[32m   4114\u001b[39m \n\u001b[32m   4115\u001b[39m \u001b[33;03mMarks the completion of a W&B run and ensures all data is synced to the server.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   4127\u001b[39m \u001b[33;03m    quiet: Deprecated. Configure logging verbosity using `wandb.Settings(quiet=...)`.\u001b[39;00m\n\u001b[32m   4128\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   4129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wandb.run:\n\u001b[32m-> \u001b[39m\u001b[32m4130\u001b[39m     \u001b[43mwandb\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data science\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:449\u001b[39m, in \u001b[36m_run_decorator._noop.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    446\u001b[39m         wandb.termwarn(message, repeat=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    447\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m.Dummy()\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data science\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:391\u001b[39m, in \u001b[36m_run_decorator._attach.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    389\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m    390\u001b[39m     \u001b[38;5;28mcls\u001b[39m._is_attaching = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data science\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:2106\u001b[39m, in \u001b[36mRun.finish\u001b[39m\u001b[34m(self, exit_code, quiet)\u001b[39m\n\u001b[32m   2098\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m quiet \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2099\u001b[39m     deprecate.deprecate(\n\u001b[32m   2100\u001b[39m         field_name=deprecate.Deprecated.run__finish_quiet,\n\u001b[32m   2101\u001b[39m         warning_message=(\n\u001b[32m   (...)\u001b[39m\u001b[32m   2104\u001b[39m         ),\n\u001b[32m   2105\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2106\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data science\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:2127\u001b[39m, in \u001b[36mRun._finish\u001b[39m\u001b[34m(self, exit_code)\u001b[39m\n\u001b[32m   2124\u001b[39m \u001b[38;5;28mself\u001b[39m._is_finished = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2126\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2127\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_atexit_cleanup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2129\u001b[39m     \u001b[38;5;66;03m# Run hooks that should happen after the last messages to the\u001b[39;00m\n\u001b[32m   2130\u001b[39m     \u001b[38;5;66;03m# internal service, like detaching the logger.\u001b[39;00m\n\u001b[32m   2131\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._teardown_hooks:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data science\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:2352\u001b[39m, in \u001b[36mRun._atexit_cleanup\u001b[39m\u001b[34m(self, exit_code)\u001b[39m\n\u001b[32m   2349\u001b[39m         os.remove(\u001b[38;5;28mself\u001b[39m._settings.resume_fname)\n\u001b[32m   2351\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2352\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_on_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2354\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   2355\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m wandb.wandb_agent._is_running():  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data science\\.venv\\Lib\\site-packages\\wandb\\sdk\\wandb_run.py:2609\u001b[39m, in \u001b[36mRun._on_finish\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2602\u001b[39m     exit_handle = \u001b[38;5;28mself\u001b[39m._backend.interface.deliver_finish_without_exit()\n\u001b[32m   2604\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m progress.progress_printer(\n\u001b[32m   2605\u001b[39m     \u001b[38;5;28mself\u001b[39m._printer,\n\u001b[32m   2606\u001b[39m     default_text=\u001b[33m\"\u001b[39m\u001b[33mFinishing up...\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2607\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m progress_printer:\n\u001b[32m   2608\u001b[39m     \u001b[38;5;66;03m# Wait for the run to complete.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2609\u001b[39m     \u001b[43mwait_with_progress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2610\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexit_handle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2611\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2612\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_after\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2613\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisplay_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunctools\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2614\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_display_finish_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2615\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_printer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2616\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2617\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2619\u001b[39m \u001b[38;5;66;03m# Print some final statistics.\u001b[39;00m\n\u001b[32m   2620\u001b[39m poll_exit_handle = \u001b[38;5;28mself\u001b[39m._backend.interface.deliver_poll_exit()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data science\\.venv\\Lib\\site-packages\\wandb\\sdk\\mailbox\\wait_with_progress.py:24\u001b[39m, in \u001b[36mwait_with_progress\u001b[39m\u001b[34m(handle, timeout, progress_after, display_progress)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwait_with_progress\u001b[39m(\n\u001b[32m     14\u001b[39m     handle: MailboxHandle[_T],\n\u001b[32m     15\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m     display_progress: Callable[[], Coroutine[Any, Any, \u001b[38;5;28;01mNone\u001b[39;00m]],\n\u001b[32m     19\u001b[39m ) -> _T:\n\u001b[32m     20\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Wait for a handle, possibly displaying progress to the user.\u001b[39;00m\n\u001b[32m     21\u001b[39m \n\u001b[32m     22\u001b[39m \u001b[33;03m    Equivalent to passing a single handle to `wait_all_with_progress`.\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwait_all_with_progress\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisplay_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisplay_progress\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data science\\.venv\\Lib\\site-packages\\wandb\\sdk\\mailbox\\wait_with_progress.py:70\u001b[39m, in \u001b[36mwait_all_with_progress\u001b[39m\u001b[34m(handle_list, timeout, progress_after, display_progress)\u001b[39m\n\u001b[32m     67\u001b[39m start_time = time.monotonic()\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wait_handles\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_after\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m:\n\u001b[32m     72\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data science\\.venv\\Lib\\site-packages\\wandb\\sdk\\mailbox\\wait_with_progress.py:110\u001b[39m, in \u001b[36m_wait_handles\u001b[39m\u001b[34m(handle_list, timeout)\u001b[39m\n\u001b[32m    108\u001b[39m     elapsed_time = time.monotonic() - start_time\n\u001b[32m    109\u001b[39m     remaining_timeout = timeout - elapsed_time\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     results.append(\u001b[43mhandle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_or\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mremaining_timeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data science\\.venv\\Lib\\site-packages\\wandb\\sdk\\mailbox\\mailbox_handle.py:122\u001b[39m, in \u001b[36m_MailboxMappedHandle.wait_or\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    121\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwait_or\u001b[39m(\u001b[38;5;28mself\u001b[39m, *, timeout: \u001b[38;5;28mfloat\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m) -> _S:\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fn(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait_or\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\data science\\.venv\\Lib\\site-packages\\wandb\\sdk\\mailbox\\response_handle.py:88\u001b[39m, in \u001b[36mMailboxResponseHandle.wait_or\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m math.isfinite(timeout):\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTimeout must be finite or None.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_event\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     89\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[32m     90\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTimed out waiting for response on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m._address\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     91\u001b[39m     )\n\u001b[32m     93\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:655\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    653\u001b[39m signaled = \u001b[38;5;28mself\u001b[39m._flag\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "wandb.init(project='test', name='fashion-MNIST samples')\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "class_lbs=['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "fig, axes=plt.subplots(2,5, figsize=(10,5))\n",
    "axes=axes.flatten()\n",
    "marked_img=[]\n",
    "\n",
    "for i in range(10):\n",
    "    idx=np.where(y_train ==i)[0][0]\n",
    "    axes[i].imshow(x_train[idx], cmap='gray')\n",
    "    axes[i].set_title(class_lbs[i])\n",
    "    axes[i].axis('off')\n",
    "    marked_img.append(wandb.Image(x_train[idx], caption=class_lbs[i]))\n",
    "\n",
    "plt.tight_layout() \n",
    "plt.show()\n",
    "#logging figure to wandb\n",
    "wandb.log({\"fashion-MNIST samples\": marked_img})\n",
    "wandb.finish()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER 2.\n",
    "\n",
    "Implement a feedforward neural network which takes images from the fashion-mnist data as input and outputs a probability distribution over the 10 classes.\n",
    "\n",
    "Your code should be flexible such that it is easy to change the number of hidden layers and the number of neurons in each hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#loading the dataset\n",
    "(x_train_full, y_train_full), (x_test,y_test)=fashion_mnist.load_data()\n",
    " \n",
    "# normalize images\n",
    "x_train_full=x_train_full.reshape(x_train_full.shape[0], -1)/255.0\n",
    "x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n",
    "#shuffle dataset\n",
    "np.random.seed(42)\n",
    "indices=np.arange(x_train_full.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "x_train_full, y_train_full=x_train_full[indices], y_train_full[indices]\n",
    "\n",
    "# splitting the dataset into training(85%) and validation(15%)\n",
    "train_size=int(.85*len(x_train_full))\n",
    "\n",
    "x_train, y_train=x_train_full[:train_size],y_train_full[:train_size]\n",
    "x_val, y_val=x_train_full[train_size:], y_train_full[train_size:]\n",
    "\n",
    "#converting truth(y) into one hot vector\n",
    "t_classes=10\n",
    "y_train_1h=np.eye(t_classes)[y_train]\n",
    "y_val_1h=np.eye(t_classes)[y_val]\n",
    "y_test_1h=np.eye(t_classes)[y_test]\n",
    "# print(len(x_test))\n",
    "\n",
    "# print(f\"Train set: {x_train.shape}, Validation set: {x_val.shape}, Test set: {x_test.shape}\") #Train set: (51000, 784), Validation set: (9000, 784), Test set: (10000, 784)\n",
    "# print(f\"\\n{y_train_1h.shape},  {y_val_1h.shape}\") #(51000, 10), (9000, 10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class feed_forward_NN2:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.layers=layers\n",
    "        self.weights=[]\n",
    "        self.biases=[]\n",
    "        self.layer_n=len(layers)\n",
    "\n",
    "        for i in range(self.layer_n-1):\n",
    "            w=np.random.randn(self.layers[i],self.layers[i+1])*np.sqrt(2/self.layers[i]) #size(self.layers[i] x self.layers[i+1])\n",
    "            b=np.zeros((1,self.layers[i+1])) #size(self.layers[i] x self.layers[i+1])\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "    def sigmoid(self,x):\n",
    "        return 1/ (1+np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x=np.exp(x-np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x/np.sum(exp_x, axis=1, keepdims=True)\n",
    "        \n",
    "    def sig_derivative(self, x):\n",
    "        return x*(1-x)\n",
    "    \n",
    "    def forward_pass(self,x):\n",
    "        self.h=[x]\n",
    "        for i in range(self.layer_n-2):\n",
    "            a=np.dot(self.h[i], self.weights[i])+ self.biases[i]\n",
    "            h=self.sigmoid(a)\n",
    "            self.h.append(h)\n",
    "        #Output_layer  _using_softmax\n",
    "        a_out=np.dot(self.h[-1],self.weights[-1]) +self.biases[-1]\n",
    "        h_out=self.softmax(a_out)\n",
    "        self.h.append(h_out)\n",
    "        return self.h\n",
    "    \n",
    "    def predicted_prob_distribution(self,x):\n",
    "        y_cap=self.forward_pass(x)\n",
    "        return y_cap[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[784, 128, 64, 10] \n",
      "\n",
      "probaability distribution over 10 classes is \n",
      " [[0.11452927 0.1957214  0.25370661 ... 0.06206364 0.11810719 0.08825023]\n",
      " [0.11847855 0.2026304  0.2574039  ... 0.06227903 0.11349867 0.08319885]\n",
      " [0.12857067 0.18869202 0.2670616  ... 0.05860327 0.11270174 0.07906325]\n",
      " ...\n",
      " [0.11940737 0.20153591 0.25191175 ... 0.06082405 0.11502835 0.08686776]\n",
      " [0.11510043 0.19273059 0.26058288 ... 0.06191339 0.12242102 0.08220833]\n",
      " [0.11903308 0.19551982 0.25559116 ... 0.06398071 0.11590744 0.07853687]]\n",
      "shape of the preditions : (60000, 10)\n"
     ]
    }
   ],
   "source": [
    "hidden_layers=[128,64]\n",
    "layers=[784]+hidden_layers+[10]\n",
    "print(layers, '\\n')\n",
    "\n",
    "network=feed_forward_NN2(layers)\n",
    "predictions=network.predicted_prob_distribution(x_train_full)\n",
    "\n",
    "print('probaability distribution over 10 classes is \\n',predictions)\n",
    "print(\"shape of the preditions :\",predictions.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER 3\n",
    "\n",
    "Implement the backpropagation algorithm with support for the following optimisation functions\n",
    "\n",
    "- **sgd**\n",
    "- **momentum based gradient descent**\n",
    "- **nesterov accelerated gradient descent**\n",
    "- **rmsprop**\n",
    "- **adam**\n",
    "- **nadam**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class feed_forward_NN_3:\n",
    "\n",
    "    def __init__(self, layers, optimizer, learning_rate, momentum, beta1, beta2, beta, epsilon):\n",
    "        \n",
    "        \n",
    "        self.layers=layers\n",
    "        self.layer_n=len(layers)\n",
    "        self.optimizer= optimizer.lower()\n",
    "        self.lr=learning_rate\n",
    "        self.momentum=momentum\n",
    "        self.beta1=beta1\n",
    "        self.beta2=beta2\n",
    "        self.beta=beta\n",
    "        self.epsilon=epsilon\n",
    "\n",
    "        self.weights=[]\n",
    "        self.biases=[]\n",
    "        \n",
    "        \n",
    "\n",
    "        for i in range(self.layer_n-1):\n",
    "            # initialization for hidden layers\n",
    "            w=np.random.randn(layers[i],layers[i+1])*np.sqrt(2.0/layers[i]) #size(self.layers[i] x self.layers[i+1])\n",
    "            b=np.zeros((1,layers[i+1])) #size(self.layers[i] x self.layers[i+1])\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "        # initialization of extra parameters for diff optimizers\n",
    "        self._init_optim_params()\n",
    "\n",
    "    def _init_optim_params(self):\n",
    "\n",
    "        # common for  momentum, NAG, Adam, etc.\n",
    "        if self.optimizer in [\"momentum\", \"nesterov\", \"adam\", \"nadam\", \"rmsprop\"]:\n",
    "            self.v_w=[np.zeros_like(w) for w in self.weights]\n",
    "            self.v_b=[np.zeros_like(b) for b in self.biases]\n",
    "\n",
    "\n",
    "        # Adam or Nadam ( first moment and time step)\n",
    "        if self.optimizer in [\"adam\",\"nadam\"]:\n",
    "            self.m_w =[np.zeros_like(w) for w in self.weights]\n",
    "            self.m_b =[np.zeros_like(b) for b in self.biases]\n",
    "            self.t =0\n",
    "\n",
    "\n",
    "    def sigmoid(self,x):\n",
    "        return 1/ (1+np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        exp_x=np.exp(x-np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x/np.sum(exp_x, axis=1, keepdims=True)\n",
    "        \n",
    "    def sig_derivative(self, x):\n",
    "        return x*(1-x)\n",
    "    \n",
    "    def forward_pass(self,x):\n",
    "        self.h=[x]\n",
    "        for i in range(self.layer_n-2):\n",
    "            a=np.dot(self.h[i], self.weights[i])+ self.biases[i]\n",
    "            h=self.sigmoid(a)\n",
    "            self.h.append(h)\n",
    "        #Output_layer  _using_softmax\n",
    "        a_out=np.dot(self.h[-1],self.weights[-1]) +self.biases[-1]\n",
    "        h_out=self.softmax(a_out)\n",
    "        self.h.append(h_out)\n",
    "        return self.h\n",
    "    \n",
    "    def backward_prop(self,y_true):\n",
    "        #batch_len=len(x)\n",
    "        m = y_true.shape[0]\n",
    "        dw = [None] * (self.layer_n - 1)\n",
    "        db = [None] * (self.layer_n - 1)\n",
    "\n",
    "        #gradient with respect to outpur layer\n",
    "        delta=self.h[-1]-y_true\n",
    "        #gradient wit respect to hidden layers\n",
    "        for i in reversed(range(self.layer_n-1)):\n",
    "            dw[i]=np.dot(self.h[i].T, delta)/m\n",
    "            db[i]=np.sum(delta,axis=0,keepdims=True)/m\n",
    "\n",
    "            if i>0:\n",
    "                delta=np.dot(delta,self.weights[i].T)*self.sig_derivative(self.h[i])\n",
    "\n",
    "        return dw, db\n",
    "    \n",
    "    def _update_params(self, dw, db):\n",
    "\n",
    "        if self.optimizer ==\"sgd\":\n",
    "            #Stochastic Gradient Descent\n",
    "            for i in range(self.layer_n -1):\n",
    "                self.weights[i]-= self.lr *dw[i]\n",
    "                self.biases[i]-= self.lr* db[i]\n",
    "\n",
    "        elif self.optimizer ==\"momentum\":\n",
    "            for i in range(self.layer_n -1):\n",
    "                self.v_w[i]= self.momentum*self.v_w[i] +dw[i]\n",
    "                self.v_b[i]= self.momentum*self.v_b[i]+db[i]\n",
    "\n",
    "                self.weights[i]-= self.lr*self.v_w[i]\n",
    "                self.biases[i] -= self.lr*self.v_b[i]               \n",
    "\n",
    "        elif self.optimizer ==\"rmsprop\":\n",
    "            for i in range(self.layer_n-1):\n",
    "                self.v_w[i] = self.beta * self.v_w[i] + (1 - self.beta) * (dw[i] ** 2)\n",
    "                self.v_b[i] = self.beta * self.v_b[i] + (1 - self.beta) * (db[i] ** 2)\n",
    "                self.weights[i] -= self.lr * dw[i] / (np.sqrt(self.v_w[i] )+ self.epsilon)\n",
    "                self.biases[i]  -= self.lr * db[i] / (np.sqrt(self.v_b[i] )+ self.epsilon)\n",
    "\n",
    "        elif self.optimizer == \"adam\":\n",
    "            self.t += 1\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * dw[i]\n",
    "                self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * db[i]\n",
    "                self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (dw[i] ** 2)\n",
    "                self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (db[i] ** 2)\n",
    "\n",
    "                # bias correction\n",
    "                m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "                m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "                v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "                v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "                self.weights[i] -= self.lr * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "                self.biases[i]  -= self.lr * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "        elif self.optimizer == \"nadam\":\n",
    "            self.t += 1\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * dw[i]\n",
    "                self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * db[i]\n",
    "                self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (dw[i] ** 2)\n",
    "                self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (db[i] ** 2)\n",
    "\n",
    "                # bias correction\n",
    "                m_w_hat = self.m_w[i] / (1 - self.beta1 ** (self.t+1))\n",
    "                m_b_hat = self.m_b[i] / (1 - self.beta1 ** (self.t+1))\n",
    "                v_w_hat = self.v_w[i] / (1 - self.beta2 ** (self.t+1))\n",
    "                v_b_hat = self.v_b[i] / (1 - self.beta2 ** (self.t+1))\n",
    "\n",
    "                # final update\n",
    "                grad_term_w = self.beta1 * m_w_hat + (1 - self.beta1) * dw[i] / (1 - self.beta1 ** (self.t+1))\n",
    "                grad_term_b = self.beta1 * m_b_hat + (1 - self.beta1) * db[i] / (1 - self.beta1 ** (self.t+1))\n",
    "\n",
    "                self.weights[i] -= self.lr * grad_term_w / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "                self.biases[i]  -= self.lr * grad_term_b / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "    \n",
    "    def _train_step(self, x_batch, y_batch):\n",
    "         \n",
    "        if self.optimizer == \"nesterov\":\n",
    "            # shit to look-ahead: w_look = w - momentum * v\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.momentum * self.v_w[i]\n",
    "                self.biases[i]  -= self.momentum * self.v_b[i]\n",
    "\n",
    "            # forward + backward at the look-ahead position\n",
    "            self.forward_pass(x_batch)\n",
    "            out = self.h[-1]\n",
    "            loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis=1)) \n",
    "            dW, dB = self.backward_prop(y_batch)\n",
    "\n",
    "            # shift back to original w_t\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] += self.momentum * self.v_w[i]\n",
    "                self.biases[i]  += self.momentum * self.v_b[i]\n",
    "\n",
    "            # update velocity: u_t = beta*u_{t-1} + grad_look\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.v_w[i] = self.momentum * self.v_w[i] + dW[i]\n",
    "                self.v_b[i] = self.momentum * self.v_b[i] + dB[i]\n",
    "\n",
    "            # final update: w_{t+1} = w_t - lr * u_t\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.lr * self.v_w[i]\n",
    "                self.biases[i]  -= self.lr * self.v_b[i]\n",
    "\n",
    "            return loss\n",
    "\n",
    "        else:\n",
    "            # Normal forward/back\n",
    "            self.forward_pass(x_batch)\n",
    "            out = self.h[-1]\n",
    "            loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis=1))\n",
    "\n",
    "            # Backward pass\n",
    "            dw, db = self.backward_prop(y_batch)\n",
    "\n",
    "            # Update parameters with the chosen optimizer\n",
    "            self._update_params(dw, db)\n",
    "\n",
    "            return loss\n",
    "\n",
    "    def training(self, x_train, y_train, x_val, y_val, epochs, batch_size,wandb_project):\n",
    "        wandb.init(project=wandb_project, config={\n",
    "            \"optimizer\": self.optimizer,\n",
    "            \"learning_rate\": self.lr,\n",
    "            \"momentum\": self.momentum,\n",
    "            \"beta1\": self.beta1,\n",
    "            \"beta2\": self.beta2,\n",
    "            \"beta\": self.beta,\n",
    "            \"epsilon\": self.epsilon,\n",
    "            \"epochs\": epochs,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"layers\": self.layers\n",
    "        })\n",
    "\n",
    "\n",
    "        #n_samples=len(x_train)\n",
    "        for ep in range(epochs):\n",
    "\n",
    "            # Shuffel data\n",
    "            idx=np.random.permutation(x_train.shape[0])\n",
    "            x_train_shuff=x_train[idx]\n",
    "            y_train_shuff=y_train[idx]\n",
    "\n",
    "            n_batches=len(x_train)// batch_size\n",
    "            batch_no=0   \n",
    "            epoch_loss=0\n",
    "\n",
    "            for b in range(n_batches):\n",
    "                start = b* batch_size\n",
    "                end =start+batch_size\n",
    "                x_batch=x_train_shuff[start: end]\n",
    "                y_batch=y_train_shuff[start: end]\n",
    "\n",
    "                # forward pass\n",
    "                loss=self._train_step(x_batch, y_batch)\n",
    "                batch_no+=1\n",
    "                epoch_loss+=loss\n",
    "\n",
    "            # average loss        \n",
    "            loss_avg=epoch_loss/batch_no\n",
    "\n",
    "            # use of validation dataset for predictions\n",
    "            valid_prediction= self.predict(x_val)\n",
    "            val_labels=np.argmax(y_val, axis =1)\n",
    "            valid_accuracy=np.mean(valid_prediction ==val_labels)\n",
    "\n",
    "            # loging in wandb\n",
    "            wandb.log({'epoch':ep+1, 'training_loss' : loss_avg, 'validation_accuracy': valid_accuracy})\n",
    "            print(f\"epoch: {ep+1}/{epochs}, train_loss:{loss_avg:.5f}, val_accuracy: {valid_accuracy:.5f}\")\n",
    "\n",
    "        wandb.finish()\n",
    "    \n",
    "\n",
    "\n",
    "    def predict(self,x):\n",
    "        y_cap=self.forward_pass(x)\n",
    "        return np.argmax(y_cap[-1], axis=1)\n",
    "    \n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    optimizer_name=\"adam\" # here you can choose any optimizer(available optimizers are 1. \"sgd\", 2. \"momentum\", 3. \"nesterov\", 4. \"rmsprop\", 5. \"adam\", 6. \"nadam\")\n",
    "\n",
    "    model =feed_forward_NN_3(\n",
    "        layers=[784, 128, 64, 10],\n",
    "        optimizer=optimizer_name,\n",
    "        learning_rate=0.001,\n",
    "        momentum=0.9,\n",
    "        beta1=0.9,\n",
    "        beta2=0.999,\n",
    "        beta=0.9,\n",
    "        epsilon=1e-4,\n",
    "    )\n",
    "    model.training(\n",
    "        x_train=x_train,\n",
    "        y_train=y_train_1h,\n",
    "        x_val=x_val,\n",
    "        y_val=y_val_1h,\n",
    "        epochs=25,\n",
    "        batch_size=64,\n",
    "        wandb_project=\"q4_sweep_project\"\n",
    "    )\n",
    "\n",
    "    test_preds = model.predict(x_test)\n",
    "    test_labels = np.argmax(y_test_1h, axis=1)\n",
    "    test_acc = np.mean(test_preds == test_labels)\n",
    "    print(\"Test Accuracy:\", test_acc)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ANSWER 4 (10 Marks)\n",
    "\n",
    "Use the sweep functionality provided by wandb to find the best values for the hyperparameters listed below. Use the standard train/test split of fashion_mnist (use (X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()). Keep 10% of the training data aside as validation data for this hyperparameter search. Here are some suggestions for different values to try for hyperparameters. As you can quickly see that this leads to an exponential number of combinations. You will have to think about strategies to do this hyperparameter search efficiently. Check out the options provided by wandb.sweep and write down what strategy you chose and why.\n",
    "\n",
    "-number of epochs: 5, 10\n",
    "-number of hidden layers: 3, 4, 5\n",
    "-size of every hidden layer: 32, 64, 128\n",
    "-weight decay (L2 regularisation): 0, 0.0005, 0.5\n",
    "-learning rate: 1e-3, 1 e-4\n",
    "-optimizer: sgd, momentum, nesterov, rmsprop, adam, nadam\n",
    "-batch size: 16, 32, 64\n",
    "-weight initialisation: random, Xavier\n",
    "-activation functions: sigmoid, tanh, ReLU\n",
    "\n",
    "#### Question 5 (5 marks)\n",
    "We would like to see the best accuracy on the validation set across all the models that you train.\n",
    "\n",
    "wandb automatically generates this plot which summarises the test accuracy of all the models that you tested. Please paste this plot below using the \"Add Panel to Report\" feature\n",
    "\n",
    "#### Question 6 (20 Marks)\n",
    "Based on the different experiments that you have run we want you to make some inferences about which configurations worked and which did not.\n",
    "\n",
    "Here again, wandb automatically generates a \"Parallel co-ordinates plot\" and a \"correlation summary\" as shown below. Learn about a \"Parallel co-ordinates plot\" and how to read it.\n",
    "\n",
    "By looking at the plots that you get, write down some interesting observations (simple bullet points but should be insightful). You can also refer to the plot in Question 5 while writing these insights. For example, in the above sample plot there are many configurations which give less than 65% accuracy. I would like to zoom into those and see what is happening.\n",
    "\n",
    "I would also like to see a recommendation for what configuration to use to get close to 95% accuracy.\n",
    "\n",
    "#### Question 7 (10 Marks)\n",
    "For the best model identified above, report the accuracy on the test set of fashion_mnist and plot the confusion matrix as shown below. More marks for creativity (less marks for producing the plot shown below as it is)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create sweep with ID: v4wd3zv6\n",
      "Sweep URL: https://wandb.ai/ed24s401-indian-institute-of-technology-madras/q4_sweep_project/sweeps/v4wd3zv6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Agent Starting Run: didgj2f6 with config:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tactivation: relu\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbatch_size: 32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta: 0.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta1: 0.9\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tbeta2: 0.999\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepochs: 10\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tepsilon: 1e-08\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \thidden_size: 128\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tinit_type: random\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tlearning_rate: 0.001\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tloss: cross_entropy\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tmomentum: 0.8\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tnum_layers: 3\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \toptimizer: momentum\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \tweight_decay: 0.5\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\acer\\OneDrive\\Documents\\GitHub\\dl_assignment_ed24s401\\wandb\\run-20250402_160651-didgj2f6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ed24s401-indian-institute-of-technology-madras/q4_sweep_project/runs/didgj2f6' target=\"_blank\">hearty-sweep-1</a></strong> to <a href='https://wandb.ai/ed24s401-indian-institute-of-technology-madras/q4_sweep_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>Sweep page: <a href='https://wandb.ai/ed24s401-indian-institute-of-technology-madras/q4_sweep_project/sweeps/v4wd3zv6' target=\"_blank\">https://wandb.ai/ed24s401-indian-institute-of-technology-madras/q4_sweep_project/sweeps/v4wd3zv6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ed24s401-indian-institute-of-technology-madras/q4_sweep_project' target=\"_blank\">https://wandb.ai/ed24s401-indian-institute-of-technology-madras/q4_sweep_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View sweep at <a href='https://wandb.ai/ed24s401-indian-institute-of-technology-madras/q4_sweep_project/sweeps/v4wd3zv6' target=\"_blank\">https://wandb.ai/ed24s401-indian-institute-of-technology-madras/q4_sweep_project/sweeps/v4wd3zv6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ed24s401-indian-institute-of-technology-madras/q4_sweep_project/runs/didgj2f6' target=\"_blank\">https://wandb.ai/ed24s401-indian-institute-of-technology-madras/q4_sweep_project/runs/didgj2f6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - loss=25.8019, val_acc=0.1012, val_loss=2.3435450887741545\n",
      "Epoch 2/10 - loss=2.3075, val_acc=0.1012, val_loss=2.302596699158068\n",
      "Epoch 3/10 - loss=2.3027, val_acc=0.1012, val_loss=2.3026028350467733\n",
      "Epoch 4/10 - loss=2.3026, val_acc=0.0977, val_loss=2.3026155897161975\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "\n",
    "# Neural Network Class: feed_forward_NN_4\n",
    "\n",
    "class feed_forward_NN_4:\n",
    "    def __init__(self,\n",
    "                 layers,\n",
    "                 optimizer,\n",
    "                 learning_rate,\n",
    "                 momentum,\n",
    "                 beta1,\n",
    "                 beta2,\n",
    "                 beta,\n",
    "                 epsilon,\n",
    "                 weight_decay,\n",
    "                 init_type,\n",
    "                 activation,\n",
    "                 loss):\n",
    "    \n",
    "        \n",
    "        self.layers = layers\n",
    "        self.layer_n = len(layers)\n",
    "        self.optimizer = optimizer.lower()\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.weight_decay = weight_decay\n",
    "        self.init_type = init_type.lower()\n",
    "        self.activation = activation.lower()\n",
    "        self.loss =loss.lower()\n",
    "\n",
    "\n",
    "                # Initialize Weights & BiaseS\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.layer_n - 1):\n",
    "            if self.init_type == \"xavier\":\n",
    "                # \"Xavier\" initialization\n",
    "                w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(1.0 / layers[i])\n",
    "            else:\n",
    "                # \"random\" initialization\n",
    "                w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])\n",
    "            b = np.zeros((1, layers[i+1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "        # initialize extra Params \n",
    "        if self.optimizer in [\"momentum\", \"nag\", \"rmsprop\", \"adam\", \"nadam\"]:\n",
    "            self.v_w = [np.zeros_like(w) for w in self.weights]\n",
    "            self.v_b = [np.zeros_like(b) for b in self.biases]\n",
    "        if self.optimizer in [\"adam\", \"nadam\"]:\n",
    "            self.m_w = [np.zeros_like(w) for w in self.weights]\n",
    "            self.m_b = [np.zeros_like(b) for b in self.biases]\n",
    "            self.t = 0\n",
    "\n",
    "    # activations \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def activate(self, x):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return self.sigmoid(x)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return self.tanh(x)\n",
    "        elif self.activation == \"relu\":\n",
    "            return self.relu(x)\n",
    "        else:\n",
    "            return self.sigmoid(x) \n",
    "        \n",
    "    # derivatives\n",
    "    def derivative(self, a):\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return a * (1 - a)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return 1 - a**2\n",
    "        elif self.activation == \"relu\":\n",
    "            return (a > 0).astype(float)\n",
    "        else:\n",
    "            return a * (1 - a) \n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward_pass(self, x):\n",
    "        self.h = [x]  \n",
    "        # Hidden layers\n",
    "        for i in range(self.layer_n - 2):\n",
    "            z = np.dot(self.h[i], self.weights[i]) + self.biases[i]\n",
    "            act = self.activate(z)\n",
    "            self.h.append(act)\n",
    "        # Output layer- softmax\n",
    "        z_out = np.dot(self.h[-1], self.weights[-1]) + self.biases[-1]\n",
    "        out = self.softmax(z_out)\n",
    "        self.h.append(out)\n",
    "        return self.h\n",
    "\n",
    "    # Backward Pass\n",
    "    def backward_prop(self, y_true):\n",
    "        m = y_true.shape[0]\n",
    "        dw = [None] * (self.layer_n - 1)\n",
    "        db = [None] * (self.layer_n - 1)\n",
    "\n",
    "        # Cross-entropy derivative for output layer\n",
    "        delta = self.h[-1] - y_true  # shape: (batch_size, output_dim)\n",
    "\n",
    "        # Propagation\n",
    "        for i in reversed(range(self.layer_n - 1)):\n",
    "            dw[i] = np.dot(self.h[i].T, delta) / m\n",
    "            db[i] = np.sum(delta, axis=0, keepdims=True) / m\n",
    "            if i > 0:\n",
    "                # For hidden layers, multiply by derivative of activation\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.derivative(self.h[i])\n",
    "        return dw, db\n",
    "\n",
    "    # Param Updates for \"Non-nag\" \n",
    "    def _update_params(self, dw, db):\n",
    "        # Add weight decay to each gradient\n",
    "        for i in range(self.layer_n - 1):\n",
    "            dw[i] += self.weight_decay * self.weights[i]\n",
    "\n",
    "        if self.optimizer == \"sgd\":\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.lr * dw[i]\n",
    "                self.biases[i] -= self.lr * db[i]\n",
    "\n",
    "        elif self.optimizer == \"momentum\":\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.v_w[i] = self.momentum * self.v_w[i] + dw[i]\n",
    "                self.v_b[i] = self.momentum * self.v_b[i] + db[i]\n",
    "                self.weights[i] -= self.lr * self.v_w[i]\n",
    "                self.biases[i] -= self.lr * self.v_b[i]\n",
    "\n",
    "        elif self.optimizer == \"rmsprop\":\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.v_w[i] = self.beta * self.v_w[i] + (1 - self.beta) * (dw[i] ** 2)\n",
    "                self.v_b[i] = self.beta * self.v_b[i] + (1 - self.beta) * (db[i] ** 2)\n",
    "                self.weights[i] -= self.lr * dw[i] / (np.sqrt(self.v_w[i]) + self.epsilon)\n",
    "                self.biases[i]  -= self.lr * db[i] / (np.sqrt(self.v_b[i]) + self.epsilon)\n",
    "\n",
    "        elif self.optimizer == \"adam\":\n",
    "            self.t += 1\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * dw[i]\n",
    "                self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * db[i]\n",
    "                self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (dw[i] ** 2)\n",
    "                self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (db[i] ** 2)\n",
    "\n",
    "                # bias correction\n",
    "                m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "                m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "                v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "                v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "                self.weights[i] -= self.lr * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "                self.biases[i]  -= self.lr * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "        elif self.optimizer == \"nadam\":\n",
    "            self.t += 1\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * dw[i]\n",
    "                self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * db[i]\n",
    "                self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (dw[i] ** 2)\n",
    "                self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (db[i] ** 2)\n",
    "\n",
    "                # bias correction\n",
    "                m_w_hat = self.m_w[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "                m_b_hat = self.m_b[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "                v_w_hat = self.v_w[i] / (1 - self.beta2 ** (self.t + 1))\n",
    "                v_b_hat = self.v_b[i] / (1 - self.beta2 ** (self.t + 1))\n",
    "\n",
    "                grad_term_w = self.beta1 * m_w_hat + (1 - self.beta1) * dw[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "                grad_term_b = self.beta1 * m_b_hat + (1 - self.beta1) * db[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "\n",
    "                self.weights[i] -= self.lr * grad_term_w / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "                self.biases[i]  -= self.lr * grad_term_b / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "    # Training Step  with \"nag\"\n",
    "    def _train_step(self, x_batch, y_batch):\n",
    "        if self.optimizer == \"nag\":\n",
    "            # to look-ahead: w_look = w - momentum * v\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.lr*self.momentum * self.v_w[i]\n",
    "                self.biases[i]  -= self.lr*self.momentum * self.v_b[i]\n",
    "\n",
    "            # Forward at the look-ahead position\n",
    "            self.forward_pass(x_batch)\n",
    "            out = self.h[-1]\n",
    "            l2_norm_weights, = 0\n",
    "            for i in range(len(self.weights)):\n",
    "                l2_norm_weights += np.sum(self.weights[i] ** 2)\n",
    "            \n",
    "                    \n",
    "            l2_norm_params = l2_norm_weights #+ l2_norm_bias\n",
    "            \n",
    "            if self.loss==\"cross_entropy\":\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params # (1e-10) to prevent underflow\n",
    "            elif self.loss==\"mean_squared_error\" :\n",
    "                loss= 0.5 * np.mean(np.sum((out - y_batch)**2, axis=1))\n",
    "            else:\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params\n",
    "            #loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis=1))\n",
    "            dW, dB = self.backward_prop(y_batch)\n",
    "\n",
    "            # add weight decay here\n",
    "            for i in range(self.layer_n - 1):\n",
    "                dW[i] += self.weight_decay * self.weights[i]\n",
    "\n",
    "            # backward at the look-ahead position (go back to w_t)\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] += self.lr*self.momentum * self.v_w[i]\n",
    "                self.biases[i]  += self.lr*self.momentum * self.v_b[i]\n",
    "\n",
    "            # update velocity: u_t = momentum*u_{t-1} + dW\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.v_w[i] = self.momentum * self.v_w[i] + dW[i]\n",
    "                self.v_b[i] = self.momentum * self.v_b[i] + dB[i]\n",
    "\n",
    "            # final param update: w = w - lr*u_t\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.lr * self.v_w[i]\n",
    "                self.biases[i]  -= self.lr * self.v_b[i]\n",
    "\n",
    "            return loss\n",
    "        else:\n",
    "            # Normal forward/back\n",
    "            self.forward_pass(x_batch)\n",
    "            out = self.h[-1]\n",
    "\n",
    "            l2_norm_weights=0\n",
    "            \n",
    "            for i in range(len(self.weights)):\n",
    "                l2_norm_weights += np.sum(self.weights[i] ** 2)\n",
    "            \n",
    "                    \n",
    "            l2_norm_params = l2_norm_weights \n",
    "            \n",
    "            if self.loss==\"cross_entropy\":\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params # (1e-10) to prevent underflow\n",
    "            elif self.loss==\"mean_squared_error\" :\n",
    "                loss= 0.5 * np.mean(np.sum((out - y_batch)**2, axis=1))\n",
    "            else:\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params    \n",
    "            \n",
    "            dW, dB = self.backward_prop(y_batch)\n",
    "            self._update_params(dW, dB)\n",
    "            return loss\n",
    "\n",
    "    # Outer Training Loop \n",
    "    def training(self, x_train, y_train, x_val, y_val, epochs, batch_size):\n",
    "       \n",
    "        for ep in range(epochs):\n",
    "            idx = np.random.permutation(x_train.shape[0])\n",
    "            x_train_shuff = x_train[idx]\n",
    "            y_train_shuff = y_train[idx]\n",
    "            n_batches = len(x_train) // batch_size\n",
    "            epoch_loss = 0.0\n",
    "            for b in range(n_batches):\n",
    "                start = b * batch_size\n",
    "                end = start + batch_size\n",
    "                x_batch = x_train_shuff[start:end]\n",
    "                y_batch = y_train_shuff[start:end]\n",
    "                loss = self._train_step(x_batch, y_batch)\n",
    "                epoch_loss += loss\n",
    "            avg_loss = epoch_loss / n_batches\n",
    "\n",
    "            # Validation\n",
    "\n",
    "            preds = self.predict(x_val)\n",
    "            val_labels = np.argmax(y_val, axis=1)\n",
    "            val_acc = np.mean(preds == val_labels)\n",
    "\n",
    "            val_outputs = self.forward_pass(x_val)[-1]\n",
    "\n",
    "            l2_norm_weights=0\n",
    "            \n",
    "            for i in range(len(self.weights)):\n",
    "                l2_norm_weights += np.sum(self.weights[i] ** 2)\n",
    "                    \n",
    "            l2_norm_params = l2_norm_weights \n",
    "\n",
    "            # Cross-entropy loss for validation\n",
    "\n",
    "            if self.loss==\"cross_entropy\":\n",
    "                val_loss = -np.mean(np.sum(y_val * np.log(val_outputs + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params # (1e-10) to prevent underflow\n",
    "                \n",
    "            # mean_mean_squared_error loss for validation\n",
    "\n",
    "            elif self.loss==\"mean_squared_error\" :\n",
    "                val_loss= 0.5 * np.mean(np.sum((val_outputs - y_val)**2, axis=1))\n",
    "            else:\n",
    "                val_loss = -np.mean(np.sum(y_val * np.log(val_outputs + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params\n",
    "\n",
    "\n",
    "            # training accuracy\n",
    "            preds_train = self.predict(x_train)\n",
    "            train_labels = np.argmax(y_train, axis=1)\n",
    "            train_acc = np.mean(preds_train == train_labels)\n",
    "\n",
    "\n",
    "            # Log metrics to wandb\n",
    "            wandb.log({\"epoch\": ep+1, \"training_loss\": avg_loss, \"validation_accuracy\": val_acc, \"validation loss\": val_loss, \"training_acuracy\": train_acc})\n",
    "            print(f\"Epoch {ep+1}/{epochs} - loss={avg_loss:.4f}, val_acc={val_acc:.4f}, val_loss={val_loss}\" )\n",
    "\n",
    "    #Prediction \n",
    "    def predict(self, X):\n",
    "        self.forward_pass(X)\n",
    "        return np.argmax(self.h[-1], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_sweep() function\n",
    "\n",
    "def train_sweep():\n",
    "    # Initialize wandb\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "\n",
    "    #custom run name from hyperparameters\n",
    "    run_name = f\"hl_{config.num_layers}_bs_{config.batch_size}_ac_{config.activation}_op_{config.optimizer}\"\n",
    "    wandb.run.name = run_name\n",
    "\n",
    "    # Load Fashion-MNIST\n",
    "    (x_train_full, y_train_full), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    x_train_full = x_train_full.reshape(x_train_full.shape[0], -1) / 255.0\n",
    "    x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n",
    "\n",
    "    np.random.seed(42)\n",
    "    idx = np.arange(x_train_full.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    x_train_full = x_train_full[idx]\n",
    "    y_train_full = y_train_full[idx]\n",
    "\n",
    "    # 90% training, 10% validation \n",
    "    train_size=int(.9*len(x_train_full))\n",
    "\n",
    "    x_train, y_train=x_train_full[:train_size],y_train_full[:train_size]\n",
    "    x_val, y_val=x_train_full[train_size:], y_train_full[train_size:]\n",
    "\n",
    "    num_classes = 10\n",
    "    y_train_1h = np.eye(num_classes)[y_train]\n",
    "    y_val_1h = np.eye(num_classes)[y_val]\n",
    "    y_test_1h = np.eye(num_classes)[y_test]\n",
    "\n",
    "    # model\n",
    "    model = feed_forward_NN_4(\n",
    "        layers=[784] + [config.hidden_size] * config.num_layers + [10],\n",
    "        optimizer=config.optimizer,\n",
    "        learning_rate=config.learning_rate,\n",
    "        momentum=config.momentum,\n",
    "        beta1=config.beta1,\n",
    "        beta2=config.beta2,\n",
    "        beta=config.beta,\n",
    "        epsilon=config.epsilon,\n",
    "        weight_decay=config.weight_decay,\n",
    "        init_type=config.init_type,\n",
    "        activation=config.activation,\n",
    "        loss=config.loss\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.training(\n",
    "        x_train=x_train,\n",
    "        y_train=y_train_1h,\n",
    "        x_val=x_val,\n",
    "        y_val=y_val_1h,\n",
    "        epochs=config.epochs,\n",
    "        batch_size=config.batch_size\n",
    "    )\n",
    "\n",
    "    #Evaluation on test set\n",
    "    test_preds = model.predict(x_test)\n",
    "    test_labels = np.argmax(y_test_1h, axis=1)\n",
    "    test_acc = np.mean(test_preds == test_labels)\n",
    "    \n",
    "    wandb.log({\"test_accuracy\": test_acc})\n",
    "    print(\"test accuracy \",test_acc)\n",
    "\n",
    "\n",
    "    # ANSWER 7 Confusion matrix\n",
    "\n",
    "    wandb.log({\"confusion_matrix\": wandb.plot.confusion_matrix(probs=None,\n",
    "                                                            y_true=test_labels,\n",
    "                                                            preds=test_preds,\n",
    "                                                            class_names=[str(i) for i in range(num_classes)])})\n",
    "\n",
    "    \n",
    "# sweep configuration\n",
    "sweep_config = {\n",
    "    \"method\": \"random\", \n",
    "    \"metric\": {\n",
    "        \"name\": \"validation_accuracy\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"epochs\": {\"values\": [5, 10]},\n",
    "        \"num_layers\": {\"values\": [3, 4, 5]},\n",
    "        \"hidden_size\": {\"values\": [32, 64, 128]},\n",
    "        \"weight_decay\": {\"values\": [0.0, 0.0005, 0.5]},\n",
    "        \"learning_rate\": {\"values\": [1e-3, 1e-4]},\n",
    "        \"optimizer\": {\"values\": [\"sgd\", \"momentum\", \"nag\", \"rmsprop\", \"adam\", \"nadam\"]},\n",
    "        \"batch_size\": {\"values\": [16, 32, 64]},\n",
    "        \"init_type\": {\"values\": [\"random\", \"xavier\"]},\n",
    "        \"activation\": {\"values\": [\"sigmoid\", \"tanh\", \"relu\"]},\n",
    "        \"momentum\": {\"values\": [0.8, 0.9]},\n",
    "        \"beta1\": {\"values\": [0.9]},\n",
    "        \"beta2\": {\"values\": [0.999]},\n",
    "        \"beta\": {\"values\": [0.9]},\n",
    "        \"epsilon\": {\"values\": [1e-8]},\n",
    "        \"loss\":{\"values\":[\"cross_entropy\", \"mean_squared_error\"]}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Running the sweep\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Creating sweep\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"q4_sweep_project\")\n",
    "    # Launching sweep agent\n",
    "    wandb.agent(sweep_id, function=train_sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8 (5 Marks)\n",
    "In all the models above you would have used cross entropy loss. Now compare the cross entropy loss with the squared error loss. I would again like to see some automatically generated plots or your own plots to convince me whether one is better than the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### RUN this for Question 8.\n",
    "import numpy as np\n",
    "import wandb\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "\n",
    "# Neural Network Class: feed_forward_NN_4\n",
    "\n",
    "class feed_forward_NN_8:\n",
    "    def __init__(self,\n",
    "                 layers,\n",
    "                 optimizer,\n",
    "                 learning_rate,\n",
    "                 momentum,\n",
    "                 beta1,\n",
    "                 beta2,\n",
    "                 beta,\n",
    "                 epsilon,\n",
    "                 weight_decay,\n",
    "                 init_type,\n",
    "                 activation,\n",
    "                 loss):\n",
    "    \n",
    "        \n",
    "        self.layers = layers\n",
    "        self.layer_n = len(layers)\n",
    "        self.optimizer = optimizer.lower()\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.weight_decay = weight_decay\n",
    "        self.init_type = init_type.lower()\n",
    "        self.activation = activation.lower()\n",
    "        self.loss =loss.lower()\n",
    "\n",
    "\n",
    "                # Initialize Weights & BiaseS\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.layer_n - 1):\n",
    "            if self.init_type == \"xavier\":\n",
    "                # \"Xavier\" initialization\n",
    "                w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(1.0 / layers[i])\n",
    "            else:\n",
    "                # \"random\" initialization\n",
    "                w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])\n",
    "            b = np.zeros((1, layers[i+1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "        # initialize extra Params \n",
    "        if self.optimizer in [\"momentum\", \"nag\", \"rmsprop\", \"adam\", \"nadam\"]:\n",
    "            self.v_w = [np.zeros_like(w) for w in self.weights]\n",
    "            self.v_b = [np.zeros_like(b) for b in self.biases]\n",
    "        if self.optimizer in [\"adam\", \"nadam\"]:\n",
    "            self.m_w = [np.zeros_like(w) for w in self.weights]\n",
    "            self.m_b = [np.zeros_like(b) for b in self.biases]\n",
    "            self.t = 0\n",
    "\n",
    "    # activations \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def activate(self, x):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return self.sigmoid(x)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return self.tanh(x)\n",
    "        elif self.activation == \"relu\":\n",
    "            return self.relu(x)\n",
    "        else:\n",
    "            return self.sigmoid(x) \n",
    "        \n",
    "    # derivatives\n",
    "    def derivative(self, a):\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return a * (1 - a)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return 1 - a**2\n",
    "        elif self.activation == \"relu\":\n",
    "            return (a > 0).astype(float)\n",
    "        else:\n",
    "            return a * (1 - a) \n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward_pass(self, x):\n",
    "        self.h = [x]  \n",
    "        # Hidden layers\n",
    "        for i in range(self.layer_n - 2):\n",
    "            z = np.dot(self.h[i], self.weights[i]) + self.biases[i]\n",
    "            act = self.activate(z)\n",
    "            self.h.append(act)\n",
    "        # Output layer- softmax\n",
    "        z_out = np.dot(self.h[-1], self.weights[-1]) + self.biases[-1]\n",
    "        out = self.softmax(z_out)\n",
    "        self.h.append(out)\n",
    "        return self.h\n",
    "\n",
    "    # Backward Pass\n",
    "    def backward_prop(self, y_true):\n",
    "        m = y_true.shape[0]\n",
    "        dw = [None] * (self.layer_n - 1)\n",
    "        db = [None] * (self.layer_n - 1)\n",
    "\n",
    "        # Cross-entropy derivative for output layer\n",
    "        delta = self.h[-1] - y_true  # shape: (batch_size, output_dim)\n",
    "\n",
    "        # Propagation\n",
    "        for i in reversed(range(self.layer_n - 1)):\n",
    "            dw[i] = np.dot(self.h[i].T, delta) / m\n",
    "            db[i] = np.sum(delta, axis=0, keepdims=True) / m\n",
    "            if i > 0:\n",
    "                # For hidden layers, multiply by derivative of activation\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.derivative(self.h[i])\n",
    "        return dw, db\n",
    "\n",
    "    # Param Updates for \"Non-nag\" \n",
    "    def _update_params(self, dw, db):\n",
    "        # Add weight decay to each gradient\n",
    "        for i in range(self.layer_n - 1):\n",
    "            dw[i] += self.weight_decay * self.weights[i]\n",
    "\n",
    "        if self.optimizer == \"sgd\":\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.lr * dw[i]\n",
    "                self.biases[i] -= self.lr * db[i]\n",
    "\n",
    "        elif self.optimizer == \"momentum\":\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.v_w[i] = self.momentum * self.v_w[i] + dw[i]\n",
    "                self.v_b[i] = self.momentum * self.v_b[i] + db[i]\n",
    "                self.weights[i] -= self.lr * self.v_w[i]\n",
    "                self.biases[i] -= self.lr * self.v_b[i]\n",
    "\n",
    "        elif self.optimizer == \"rmsprop\":\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.v_w[i] = self.beta * self.v_w[i] + (1 - self.beta) * (dw[i] ** 2)\n",
    "                self.v_b[i] = self.beta * self.v_b[i] + (1 - self.beta) * (db[i] ** 2)\n",
    "                self.weights[i] -= self.lr * dw[i] / (np.sqrt(self.v_w[i]) + self.epsilon)\n",
    "                self.biases[i]  -= self.lr * db[i] / (np.sqrt(self.v_b[i]) + self.epsilon)\n",
    "\n",
    "        elif self.optimizer == \"adam\":\n",
    "            self.t += 1\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * dw[i]\n",
    "                self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * db[i]\n",
    "                self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (dw[i] ** 2)\n",
    "                self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (db[i] ** 2)\n",
    "\n",
    "                # bias correction\n",
    "                m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "                m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "                v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "                v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "                self.weights[i] -= self.lr * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "                self.biases[i]  -= self.lr * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "        elif self.optimizer == \"nadam\":\n",
    "            self.t += 1\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * dw[i]\n",
    "                self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * db[i]\n",
    "                self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (dw[i] ** 2)\n",
    "                self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (db[i] ** 2)\n",
    "\n",
    "                # bias correction\n",
    "                m_w_hat = self.m_w[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "                m_b_hat = self.m_b[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "                v_w_hat = self.v_w[i] / (1 - self.beta2 ** (self.t + 1))\n",
    "                v_b_hat = self.v_b[i] / (1 - self.beta2 ** (self.t + 1))\n",
    "\n",
    "                grad_term_w = self.beta1 * m_w_hat + (1 - self.beta1) * dw[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "                grad_term_b = self.beta1 * m_b_hat + (1 - self.beta1) * db[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "\n",
    "                self.weights[i] -= self.lr * grad_term_w / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "                self.biases[i]  -= self.lr * grad_term_b / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "    # Training Step  with \"nag\"\n",
    "    def _train_step(self, x_batch, y_batch):\n",
    "        if self.optimizer == \"nag\":\n",
    "            # to look-ahead: w_look = w - momentum * v\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.lr*self.momentum * self.v_w[i]\n",
    "                self.biases[i]  -= self.lr*self.momentum * self.v_b[i]\n",
    "\n",
    "            # Forward at the look-ahead position\n",
    "            self.forward_pass(x_batch)\n",
    "            out = self.h[-1]\n",
    "            l2_norm_weights, = 0\n",
    "            for i in range(len(self.weights)):\n",
    "                l2_norm_weights += np.sum(self.weights[i] ** 2)\n",
    "            \n",
    "                    \n",
    "            l2_norm_params = l2_norm_weights #+ l2_norm_bias\n",
    "            \n",
    "            if self.loss==\"cross_entropy\":\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params # (1e-10) to prevent underflow\n",
    "            elif self.loss==\"mean_squared_error\" :\n",
    "                loss= 0.5 * np.mean(np.sum((out - y_batch)**2, axis=1))\n",
    "            else:\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params\n",
    "            #loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis=1))\n",
    "            dW, dB = self.backward_prop(y_batch)\n",
    "\n",
    "            # add weight decay here\n",
    "            for i in range(self.layer_n - 1):\n",
    "                dW[i] += self.weight_decay * self.weights[i]\n",
    "\n",
    "            # backward at the look-ahead position (go back to w_t)\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] += self.lr*self.momentum * self.v_w[i]\n",
    "                self.biases[i]  += self.lr*self.momentum * self.v_b[i]\n",
    "\n",
    "            # update velocity: u_t = momentum*u_{t-1} + dW\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.v_w[i] = self.momentum * self.v_w[i] + dW[i]\n",
    "                self.v_b[i] = self.momentum * self.v_b[i] + dB[i]\n",
    "\n",
    "            # final param update: w = w - lr*u_t\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.lr * self.v_w[i]\n",
    "                self.biases[i]  -= self.lr * self.v_b[i]\n",
    "\n",
    "            return loss\n",
    "        else:\n",
    "            # Normal forward/back\n",
    "            self.forward_pass(x_batch)\n",
    "            out = self.h[-1]\n",
    "\n",
    "            l2_norm_weights=0\n",
    "            \n",
    "            for i in range(len(self.weights)):\n",
    "                l2_norm_weights += np.sum(self.weights[i] ** 2)\n",
    "            \n",
    "                    \n",
    "            l2_norm_params = l2_norm_weights \n",
    "            \n",
    "            if self.loss==\"cross_entropy\":\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params # (1e-10) to prevent underflow\n",
    "            elif self.loss==\"mean_squared_error\" :\n",
    "                loss= 0.5 * np.mean(np.sum((out - y_batch)**2, axis=1))\n",
    "            else:\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params    \n",
    "            \n",
    "            dW, dB = self.backward_prop(y_batch)\n",
    "            self._update_params(dW, dB)\n",
    "            return loss\n",
    "\n",
    "    # Outer Training Loop \n",
    "    def training(self, x_train, y_train, x_val, y_val, epochs, batch_size):\n",
    "       \n",
    "        for ep in range(epochs):\n",
    "            idx = np.random.permutation(x_train.shape[0])\n",
    "            x_train_shuff = x_train[idx]\n",
    "            y_train_shuff = y_train[idx]\n",
    "            n_batches = len(x_train) // batch_size\n",
    "            epoch_loss = 0.0\n",
    "            for b in range(n_batches):\n",
    "                start = b * batch_size\n",
    "                end = start + batch_size\n",
    "                x_batch = x_train_shuff[start:end]\n",
    "                y_batch = y_train_shuff[start:end]\n",
    "                loss = self._train_step(x_batch, y_batch)\n",
    "                epoch_loss += loss\n",
    "            avg_loss = epoch_loss / n_batches\n",
    "\n",
    "            # Validation\n",
    "\n",
    "            preds = self.predict(x_val)\n",
    "            val_labels = np.argmax(y_val, axis=1)\n",
    "            val_acc = np.mean(preds == val_labels)\n",
    "\n",
    "            val_outputs = self.forward_pass(x_val)[-1]\n",
    "\n",
    "            l2_norm_weights=0\n",
    "            \n",
    "            for i in range(len(self.weights)):\n",
    "                l2_norm_weights += np.sum(self.weights[i] ** 2)\n",
    "                    \n",
    "            l2_norm_params = l2_norm_weights \n",
    "\n",
    "            # Cross-entropy loss for validation\n",
    "\n",
    "            if self.loss==\"cross_entropy\":\n",
    "                val_loss = -np.mean(np.sum(y_val * np.log(val_outputs + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params # (1e-10) to prevent underflow\n",
    "                \n",
    "            # mean_mean_squared_error loss for validation\n",
    "\n",
    "            elif self.loss==\"mean_squared_error\" :\n",
    "                val_loss= 0.5 * np.mean(np.sum((val_outputs - y_val)**2, axis=1))\n",
    "            else:\n",
    "                val_loss = -np.mean(np.sum(y_val * np.log(val_outputs + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params\n",
    "\n",
    "\n",
    "            # training accuracy\n",
    "            preds_train = self.predict(x_train)\n",
    "            train_labels = np.argmax(y_train, axis=1)\n",
    "            train_acc = np.mean(preds_train == train_labels)\n",
    "\n",
    "\n",
    "            # Log metrics to wandb\n",
    "            wandb.log({\"epoch\": ep+1, \"training_loss\": avg_loss, \"validation_accuracy\": val_acc, \"validation loss\": val_loss, \"training_acuracy\": train_acc})\n",
    "            print(f\"Epoch {ep+1}/{epochs} - loss={avg_loss:.4f}, val_acc={val_acc:.4f}, val_loss={val_loss}\" )\n",
    "\n",
    "    #Prediction \n",
    "    def predict(self, X):\n",
    "        self.forward_pass(X)\n",
    "        return np.argmax(self.h[-1], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_sweep() function\n",
    "\n",
    "def train_sweep():\n",
    "    # Initialize wandb\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "\n",
    "    #custom run name from hyperparameters\n",
    "    run_name = f\"hl_{config.num_layers}_bs_{config.batch_size}_ac_{config.activation}_op_{config.optimizer}\"\n",
    "    wandb.run.name = run_name\n",
    "\n",
    "    # Load Fashion-MNIST\n",
    "    (x_train_full, y_train_full), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    x_train_full = x_train_full.reshape(x_train_full.shape[0], -1) / 255.0\n",
    "    x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n",
    "\n",
    "    np.random.seed(42)\n",
    "    idx = np.arange(x_train_full.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    x_train_full = x_train_full[idx]\n",
    "    y_train_full = y_train_full[idx]\n",
    "\n",
    "    # 90% training, 10% validation \n",
    "    train_size=int(.9*len(x_train_full))\n",
    "\n",
    "    x_train, y_train=x_train_full[:train_size],y_train_full[:train_size]\n",
    "    x_val, y_val=x_train_full[train_size:], y_train_full[train_size:]\n",
    "\n",
    "    num_classes = 10\n",
    "    y_train_1h = np.eye(num_classes)[y_train]\n",
    "    y_val_1h = np.eye(num_classes)[y_val]\n",
    "    y_test_1h = np.eye(num_classes)[y_test]\n",
    "\n",
    "    # model\n",
    "    model = feed_forward_NN_8(\n",
    "        layers=[784] + [config.hidden_size] * config.num_layers + [10],\n",
    "        optimizer=config.optimizer,\n",
    "        learning_rate=config.learning_rate,\n",
    "        momentum=config.momentum,\n",
    "        beta1=config.beta1,\n",
    "        beta2=config.beta2,\n",
    "        beta=config.beta,\n",
    "        epsilon=config.epsilon,\n",
    "        weight_decay=config.weight_decay,\n",
    "        init_type=config.init_type,\n",
    "        activation=config.activation,\n",
    "        loss=config.loss\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.training(\n",
    "        x_train=x_train,\n",
    "        y_train=y_train_1h,\n",
    "        x_val=x_val,\n",
    "        y_val=y_val_1h,\n",
    "        epochs=config.epochs,\n",
    "        batch_size=config.batch_size\n",
    "    )\n",
    "\n",
    "    #Evaluation on test set\n",
    "    test_preds = model.predict(x_test)\n",
    "    test_labels = np.argmax(y_test_1h, axis=1)\n",
    "    test_acc = np.mean(test_preds == test_labels)\n",
    "    \n",
    "    wandb.log({\"test_accuracy\": test_acc})\n",
    "    print(\"test accuracy \",test_acc)\n",
    "\n",
    "\n",
    "   \n",
    "\n",
    "# sweep configuration for QUESTION 8\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"random\", \n",
    "    \"metric\": {\n",
    "        \"name\": \"validation_accuracy\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"epochs\": {\"values\": [5]},\n",
    "        \"num_layers\": {\"values\": [3]},\n",
    "        \"hidden_size\": {\"values\": [32]},\n",
    "        \"weight_decay\": {\"values\": [0.0]},\n",
    "        \"learning_rate\": {\"values\": [1e-3]},\n",
    "        \"optimizer\": {\"values\": [\"adam\"]},\n",
    "        \"batch_size\": {\"values\": [32]},\n",
    "        \"init_type\": {\"values\": [\"xavier\"]},\n",
    "        \"activation\": {\"values\": [\"relu\"]},\n",
    "        \"momentum\": {\"values\": [0.9]},\n",
    "        \"beta1\": {\"values\": [0.9]},\n",
    "        \"beta2\": {\"values\": [0.999]},\n",
    "        \"beta\": {\"values\": [0.9]},\n",
    "        \"epsilon\": {\"values\": [1e-8]},\n",
    "        \"loss\":{\"values\":[\"cross_entropy\", \"mean_squared_error\"]}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Running the sweep\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Creating sweep\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"q4_sweep_project\")\n",
    "    # Launching sweep agent\n",
    "    wandb.agent(sweep_id, function=train_sweep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10 (10 Marks)\n",
    "Based on your learnings above, give me 3 recommendations for what would work for the MNIST dataset (not Fashion-MNIST). Just to be clear, I am asking you to take your learnings based on extensive experimentation with one dataset and see if these learnings help on another dataset. If I give you a budget of running only 3 hyperparameter configurations as opposed to the large number of experiments you have run above then which 3 would you use and why. Report the accuracies that you obtain using these 3 configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "from keras.datasets import mnist\n",
    "\n",
    "\n",
    "\n",
    "# Neural Network Class: feed_forward_NN_4\n",
    "\n",
    "class feed_forward_NN_4:\n",
    "    def __init__(self,\n",
    "                 layers,\n",
    "                 optimizer,\n",
    "                 learning_rate,\n",
    "                 momentum,\n",
    "                 beta1,\n",
    "                 beta2,\n",
    "                 beta,\n",
    "                 epsilon,\n",
    "                 weight_decay,\n",
    "                 init_type,\n",
    "                 activation,\n",
    "                 loss):\n",
    "    \n",
    "        \n",
    "        self.layers = layers\n",
    "        self.layer_n = len(layers)\n",
    "        self.optimizer = optimizer.lower()\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.beta = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.weight_decay = weight_decay\n",
    "        self.init_type = init_type.lower()\n",
    "        self.activation = activation.lower()\n",
    "        self.loss =loss.lower()\n",
    "\n",
    "\n",
    "                # Initialize Weights & BiaseS\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.layer_n - 1):\n",
    "            if self.init_type == \"xavier\":\n",
    "                # \"Xavier\" initialization\n",
    "                w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(1.0 / layers[i])\n",
    "            else:\n",
    "                # \"random\" initialization\n",
    "                w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])\n",
    "            b = np.zeros((1, layers[i+1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "        # initialize extra Params \n",
    "        if self.optimizer in [\"momentum\", \"nag\", \"rmsprop\", \"adam\", \"nadam\"]:\n",
    "            self.v_w = [np.zeros_like(w) for w in self.weights]\n",
    "            self.v_b = [np.zeros_like(b) for b in self.biases]\n",
    "        if self.optimizer in [\"adam\", \"nadam\"]:\n",
    "            self.m_w = [np.zeros_like(w) for w in self.weights]\n",
    "            self.m_b = [np.zeros_like(b) for b in self.biases]\n",
    "            self.t = 0\n",
    "\n",
    "    # activations \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def activate(self, x):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return self.sigmoid(x)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return self.tanh(x)\n",
    "        elif self.activation == \"relu\":\n",
    "            return self.relu(x)\n",
    "        else:\n",
    "            return self.sigmoid(x) \n",
    "        \n",
    "    # derivatives\n",
    "    def derivative(self, a):\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return a * (1 - a)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return 1 - a**2\n",
    "        elif self.activation == \"relu\":\n",
    "            return (a > 0).astype(float)\n",
    "        else:\n",
    "            return a * (1 - a) \n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward_pass(self, x):\n",
    "        self.h = [x]  \n",
    "        # Hidden layers\n",
    "        for i in range(self.layer_n - 2):\n",
    "            z = np.dot(self.h[i], self.weights[i]) + self.biases[i]\n",
    "            act = self.activate(z)\n",
    "            self.h.append(act)\n",
    "        # Output layer- softmax\n",
    "        z_out = np.dot(self.h[-1], self.weights[-1]) + self.biases[-1]\n",
    "        out = self.softmax(z_out)\n",
    "        self.h.append(out)\n",
    "        return self.h\n",
    "\n",
    "    # Backward Pass\n",
    "    def backward_prop(self, y_true):\n",
    "        m = y_true.shape[0]\n",
    "        dw = [None] * (self.layer_n - 1)\n",
    "        db = [None] * (self.layer_n - 1)\n",
    "\n",
    "        # Cross-entropy derivative for output layer\n",
    "        delta = self.h[-1] - y_true  # shape: (batch_size, output_dim)\n",
    "\n",
    "        # Propagation\n",
    "        for i in reversed(range(self.layer_n - 1)):\n",
    "            dw[i] = np.dot(self.h[i].T, delta) / m\n",
    "            db[i] = np.sum(delta, axis=0, keepdims=True) / m\n",
    "            if i > 0:\n",
    "                # For hidden layers, multiply by derivative of activation\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.derivative(self.h[i])\n",
    "        return dw, db\n",
    "\n",
    "    # Param Updates for \"Non-nag\" \n",
    "    def _update_params(self, dw, db):\n",
    "        # Add weight decay to each gradient\n",
    "        for i in range(self.layer_n - 1):\n",
    "            dw[i] += self.weight_decay * self.weights[i]\n",
    "\n",
    "        if self.optimizer == \"sgd\":\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.lr * dw[i]\n",
    "                self.biases[i] -= self.lr * db[i]\n",
    "\n",
    "        elif self.optimizer == \"momentum\":\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.v_w[i] = self.momentum * self.v_w[i] + dw[i]\n",
    "                self.v_b[i] = self.momentum * self.v_b[i] + db[i]\n",
    "                self.weights[i] -= self.lr * self.v_w[i]\n",
    "                self.biases[i] -= self.lr * self.v_b[i]\n",
    "\n",
    "        elif self.optimizer == \"rmsprop\":\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.v_w[i] = self.beta * self.v_w[i] + (1 - self.beta) * (dw[i] ** 2)\n",
    "                self.v_b[i] = self.beta * self.v_b[i] + (1 - self.beta) * (db[i] ** 2)\n",
    "                self.weights[i] -= self.lr * dw[i] / (np.sqrt(self.v_w[i]) + self.epsilon)\n",
    "                self.biases[i]  -= self.lr * db[i] / (np.sqrt(self.v_b[i]) + self.epsilon)\n",
    "\n",
    "        elif self.optimizer == \"adam\":\n",
    "            self.t += 1\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * dw[i]\n",
    "                self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * db[i]\n",
    "                self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (dw[i] ** 2)\n",
    "                self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (db[i] ** 2)\n",
    "\n",
    "                # bias correction\n",
    "                m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "                m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "                v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "                v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "                self.weights[i] -= self.lr * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "                self.biases[i]  -= self.lr * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "        elif self.optimizer == \"nadam\":\n",
    "            self.t += 1\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * dw[i]\n",
    "                self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * db[i]\n",
    "                self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (dw[i] ** 2)\n",
    "                self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (db[i] ** 2)\n",
    "\n",
    "                # bias correction\n",
    "                m_w_hat = self.m_w[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "                m_b_hat = self.m_b[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "                v_w_hat = self.v_w[i] / (1 - self.beta2 ** (self.t + 1))\n",
    "                v_b_hat = self.v_b[i] / (1 - self.beta2 ** (self.t + 1))\n",
    "\n",
    "                grad_term_w = self.beta1 * m_w_hat + (1 - self.beta1) * dw[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "                grad_term_b = self.beta1 * m_b_hat + (1 - self.beta1) * db[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "\n",
    "                self.weights[i] -= self.lr * grad_term_w / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "                self.biases[i]  -= self.lr * grad_term_b / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "    # Training Step  with \"nag\"\n",
    "    def _train_step(self, x_batch, y_batch):\n",
    "        if self.optimizer == \"nag\":\n",
    "            # to look-ahead: w_look = w - momentum * v\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.lr*self.momentum * self.v_w[i]\n",
    "                self.biases[i]  -= self.lr*self.momentum * self.v_b[i]\n",
    "\n",
    "            # Forward at the look-ahead position\n",
    "            self.forward_pass(x_batch)\n",
    "            out = self.h[-1]\n",
    "            l2_norm_weights, = 0\n",
    "            for i in range(len(self.weights)):\n",
    "                l2_norm_weights += np.sum(self.weights[i] ** 2)\n",
    "            \n",
    "                    \n",
    "            l2_norm_params = l2_norm_weights #+ l2_norm_bias\n",
    "            \n",
    "            if self.loss==\"cross_entropy\":\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params # (1e-10) to prevent underflow\n",
    "            elif self.loss==\"mean_squared_error\" :\n",
    "                loss= 0.5 * np.mean(np.sum((out - y_batch)**2, axis=1))\n",
    "            else:\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params\n",
    "            #loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis=1))\n",
    "            dW, dB = self.backward_prop(y_batch)\n",
    "\n",
    "            # add weight decay here\n",
    "            for i in range(self.layer_n - 1):\n",
    "                dW[i] += self.weight_decay * self.weights[i]\n",
    "\n",
    "            # backward at the look-ahead position (go back to w_t)\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] += self.lr*self.momentum * self.v_w[i]\n",
    "                self.biases[i]  += self.lr*self.momentum * self.v_b[i]\n",
    "\n",
    "            # update velocity: u_t = momentum*u_{t-1} + dW\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.v_w[i] = self.momentum * self.v_w[i] + dW[i]\n",
    "                self.v_b[i] = self.momentum * self.v_b[i] + dB[i]\n",
    "\n",
    "            # final param update: w = w - lr*u_t\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.lr * self.v_w[i]\n",
    "                self.biases[i]  -= self.lr * self.v_b[i]\n",
    "\n",
    "            return loss\n",
    "        else:\n",
    "            # Normal forward/back\n",
    "            self.forward_pass(x_batch)\n",
    "            out = self.h[-1]\n",
    "\n",
    "            l2_norm_weights=0\n",
    "            \n",
    "            for i in range(len(self.weights)):\n",
    "                l2_norm_weights += np.sum(self.weights[i] ** 2)\n",
    "            \n",
    "                    \n",
    "            l2_norm_params = l2_norm_weights \n",
    "            \n",
    "            if self.loss==\"cross_entropy\":\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params # (1e-10) to prevent underflow\n",
    "            elif self.loss==\"mean_squared_error\" :\n",
    "                loss= 0.5 * np.mean(np.sum((out - y_batch)**2, axis=1))\n",
    "            else:\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params    \n",
    "            \n",
    "            dW, dB = self.backward_prop(y_batch)\n",
    "            self._update_params(dW, dB)\n",
    "            return loss\n",
    "\n",
    "    # Outer Training Loop \n",
    "    def training(self, x_train, y_train, x_val, y_val, epochs, batch_size):\n",
    "       \n",
    "        for ep in range(epochs):\n",
    "            idx = np.random.permutation(x_train.shape[0])\n",
    "            x_train_shuff = x_train[idx]\n",
    "            y_train_shuff = y_train[idx]\n",
    "            n_batches = len(x_train) // batch_size\n",
    "            epoch_loss = 0.0\n",
    "            for b in range(n_batches):\n",
    "                start = b * batch_size\n",
    "                end = start + batch_size\n",
    "                x_batch = x_train_shuff[start:end]\n",
    "                y_batch = y_train_shuff[start:end]\n",
    "                loss = self._train_step(x_batch, y_batch)\n",
    "                epoch_loss += loss\n",
    "            avg_loss = epoch_loss / n_batches\n",
    "\n",
    "            # Validation\n",
    "\n",
    "            preds = self.predict(x_val)\n",
    "            val_labels = np.argmax(y_val, axis=1)\n",
    "            val_acc = np.mean(preds == val_labels)\n",
    "\n",
    "            val_outputs = self.forward_pass(x_val)[-1]\n",
    "\n",
    "            l2_norm_weights=0\n",
    "            \n",
    "            for i in range(len(self.weights)):\n",
    "                l2_norm_weights += np.sum(self.weights[i] ** 2)\n",
    "                    \n",
    "            l2_norm_params = l2_norm_weights \n",
    "\n",
    "            # Cross-entropy loss for validation\n",
    "\n",
    "            if self.loss==\"cross_entropy\":\n",
    "                val_loss = -np.mean(np.sum(y_val * np.log(val_outputs + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params # (1e-10) to prevent underflow\n",
    "                \n",
    "            # mean_mean_squared_error loss for validation\n",
    "\n",
    "            elif self.loss==\"mean_squared_error\" :\n",
    "                val_loss= 0.5 * np.mean(np.sum((val_outputs - y_val)**2, axis=1))\n",
    "            else:\n",
    "                val_loss = -np.mean(np.sum(y_val * np.log(val_outputs + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params\n",
    "\n",
    "\n",
    "            # training accuracy\n",
    "            preds_train = self.predict(x_train)\n",
    "            train_labels = np.argmax(y_train, axis=1)\n",
    "            train_acc = np.mean(preds_train == train_labels)\n",
    "\n",
    "\n",
    "            # Log metrics to wandb\n",
    "            wandb.log({\"epoch\": ep+1, \"training_loss\": avg_loss, \"validation_accuracy\": val_acc, \"validation loss\": val_loss, \"training_acuracy\": train_acc})\n",
    "            print(f\"Epoch {ep+1}/{epochs} - loss={avg_loss:.4f}, val_acc={val_acc:.4f}, val_loss={val_loss}\" )\n",
    "\n",
    "    #Prediction \n",
    "    def predict(self, X):\n",
    "        self.forward_pass(X)\n",
    "        return np.argmax(self.h[-1], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_sweep() function\n",
    "\n",
    "def train_sweep():\n",
    "    # Initialize wandb\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "\n",
    "    #custom run name from hyperparameters\n",
    "    run_name = f\"hl_{config.num_layers}_bs_{config.batch_size}_ac_{config.activation}_op_{config.optimizer}\"\n",
    "    wandb.run.name = run_name\n",
    "\n",
    "    # Load Fashion-MNIST\n",
    "    (x_train_full, y_train_full), (x_test, y_test) = mnist.load_data()\n",
    "    x_train_full = x_train_full.reshape(x_train_full.shape[0], -1) / 255.0\n",
    "    x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n",
    "\n",
    "    np.random.seed(42)\n",
    "    idx = np.arange(x_train_full.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    x_train_full = x_train_full[idx]\n",
    "    y_train_full = y_train_full[idx]\n",
    "\n",
    "    # 90% training, 10% validation \n",
    "    train_size=int(.9*len(x_train_full))\n",
    "\n",
    "    x_train, y_train=x_train_full[:train_size],y_train_full[:train_size]\n",
    "    x_val, y_val=x_train_full[train_size:], y_train_full[train_size:]\n",
    "\n",
    "    num_classes = 10\n",
    "    y_train_1h = np.eye(num_classes)[y_train]\n",
    "    y_val_1h = np.eye(num_classes)[y_val]\n",
    "    y_test_1h = np.eye(num_classes)[y_test]\n",
    "\n",
    "    # model\n",
    "    model = feed_forward_NN_4(\n",
    "        layers=[784] + [config.hidden_size] * config.num_layers + [10],\n",
    "        optimizer=config.optimizer,\n",
    "        learning_rate=config.learning_rate,\n",
    "        momentum=config.momentum,\n",
    "        beta1=config.beta1,\n",
    "        beta2=config.beta2,\n",
    "        beta=config.beta,\n",
    "        epsilon=config.epsilon,\n",
    "        weight_decay=config.weight_decay,\n",
    "        init_type=config.init_type,\n",
    "        activation=config.activation,\n",
    "        loss=config.loss\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.training(\n",
    "        x_train=x_train,\n",
    "        y_train=y_train_1h,\n",
    "        x_val=x_val,\n",
    "        y_val=y_val_1h,\n",
    "        epochs=config.epochs,\n",
    "        batch_size=config.batch_size\n",
    "    )\n",
    "\n",
    "    #Evaluation on test set\n",
    "    test_preds = model.predict(x_test)\n",
    "    test_labels = np.argmax(y_test_1h, axis=1)\n",
    "    test_acc = np.mean(test_preds == test_labels)\n",
    "    \n",
    "    wandb.log({\"test_accuracy\": test_acc})\n",
    "    print(\"test accuracy \",test_acc)\n",
    "\n",
    "\n",
    "    # ANSWER 7 Confusion matrix\n",
    "\n",
    "    wandb.log({\"confusion_matrix\": wandb.plot.confusion_matrix(probs=None,\n",
    "                                                            y_true=test_labels,\n",
    "                                                            preds=test_preds,\n",
    "                                                            class_names=[str(i) for i in range(num_classes)])})\n",
    "\n",
    "# sweep configuration\n",
    "sweep_config = {\n",
    "    \"method\": \"random\", \n",
    "    \"metric\": {\n",
    "        \"name\": \"validation_accuracy\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"epochs\": {\"values\": [5, 10]},\n",
    "        \"num_layers\": {\"values\": [3, 4, 5]},\n",
    "        \"hidden_size\": {\"values\": [32, 64, 128]},\n",
    "        \"weight_decay\": {\"values\": [0.0, 0.0005, 0.5]},\n",
    "        \"learning_rate\": {\"values\": [1e-3, 1e-4]},\n",
    "        \"optimizer\": {\"values\": [\"sgd\", \"momentum\", \"nag\", \"rmsprop\", \"adam\", \"nadam\"]},\n",
    "        \"batch_size\": {\"values\": [16, 32, 64]},\n",
    "        \"init_type\": {\"values\": [\"random\", \"xavier\"]},\n",
    "        \"activation\": {\"values\": [\"sigmoid\", \"tanh\", \"relu\"]},\n",
    "        \"momentum\": {\"values\": [0.8, 0.9]},\n",
    "        \"beta1\": {\"values\": [0.9]},\n",
    "        \"beta2\": {\"values\": [0.999]},\n",
    "        \"beta\": {\"values\": [0.9]},\n",
    "        \"epsilon\": {\"values\": [1e-8]},\n",
    "        \"loss\":{\"values\":[\"cross_entropy\", \"mean_squared_error\"]}\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Running the sweep\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Creating sweep\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"q4_sweep_project\")\n",
    "    # Launching sweep agent\n",
    "    wandb.agent(sweep_id, function=train_sweep(), count=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
