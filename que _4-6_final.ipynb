{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import wandb\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "\n",
    "# Neural Network Class: feed_forward_NN_4\n",
    "\n",
    "class feed_forward_NN_4:\n",
    "    def __init__(self,\n",
    "                 layers,\n",
    "                 optimizer,\n",
    "                 learning_rate,\n",
    "                 momentum,\n",
    "                 beta1,\n",
    "                 beta2,\n",
    "                 beta_rms,\n",
    "                 epsilon,\n",
    "                 weight_decay,\n",
    "                 init_type,\n",
    "                 activation\n",
    "                 ):\n",
    "    \n",
    "        \n",
    "        self.layers = layers\n",
    "        self.layer_n = len(layers)\n",
    "        self.optimizer = optimizer.lower()\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.beta_rms = beta_rms\n",
    "        self.epsilon = epsilon\n",
    "        self.weight_decay = weight_decay\n",
    "        self.init_type = init_type.lower()\n",
    "        self.activation = activation.lower()\n",
    "        \n",
    "\n",
    "        # Initialize Weights & BiaseS\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.layer_n - 1):\n",
    "            if self.init_type == \"xavier\":\n",
    "                # \"Xavier\" initialization\n",
    "                w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(1.0 / layers[i])\n",
    "            else:\n",
    "                # \"random\" initialization\n",
    "                w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])\n",
    "            b = np.zeros((1, layers[i+1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "        # initialize extra Params \n",
    "        if self.optimizer in [\"momentum\", \"nesterov\", \"rmsprop\", \"adam\", \"nadam\"]:\n",
    "            self.v_w = [np.zeros_like(w) for w in self.weights]\n",
    "            self.v_b = [np.zeros_like(b) for b in self.biases]\n",
    "        if self.optimizer in [\"adam\", \"nadam\"]:\n",
    "            self.m_w = [np.zeros_like(w) for w in self.weights]\n",
    "            self.m_b = [np.zeros_like(b) for b in self.biases]\n",
    "            self.t = 0\n",
    "\n",
    "    # activations \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def activate(self, x):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return self.sigmoid(x)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return self.tanh(x)\n",
    "        elif self.activation == \"relu\":\n",
    "            return self.relu(x)\n",
    "        else:\n",
    "            return self.sigmoid(x) \n",
    "        \n",
    "    # derivatives\n",
    "    def derivative(self, a):\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return a * (1 - a)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return 1 - a**2\n",
    "        elif self.activation == \"relu\":\n",
    "            return (a > 0).astype(float)\n",
    "        else:\n",
    "            return a * (1 - a) \n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward_pass(self, x):\n",
    "        self.h = [x]  \n",
    "        # Hidden layers\n",
    "        for i in range(self.layer_n - 2):\n",
    "            z = np.dot(self.h[i], self.weights[i]) + self.biases[i]\n",
    "            act = self.activate(z)\n",
    "            self.h.append(act)\n",
    "        # Output layer- softmax\n",
    "        z_out = np.dot(self.h[-1], self.weights[-1]) + self.biases[-1]\n",
    "        out = self.softmax(z_out)\n",
    "        self.h.append(out)\n",
    "        return self.h\n",
    "\n",
    "    # Backward Pass\n",
    "    def backward_prop(self, y_true):\n",
    "        m = y_true.shape[0]\n",
    "        dw = [None] * (self.layer_n - 1)\n",
    "        db = [None] * (self.layer_n - 1)\n",
    "\n",
    "        # Cross-entropy derivative for output layer\n",
    "        delta = self.h[-1] - y_true  # shape: (batch_size, output_dim)\n",
    "\n",
    "        # Propagation\n",
    "        for i in reversed(range(self.layer_n - 1)):\n",
    "            dw[i] = np.dot(self.h[i].T, delta) / m\n",
    "            db[i] = np.sum(delta, axis=0, keepdims=True) / m\n",
    "            if i > 0:\n",
    "                # For hidden layers, multiply by derivative of activation\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.derivative(self.h[i])\n",
    "        return dw, db\n",
    "\n",
    "    # Param Updates for \"Non-Nesterov\" \n",
    "    def _update_params(self, dw, db):\n",
    "        # Add weight decay to each gradient\n",
    "        for i in range(self.layer_n - 1):\n",
    "            dw[i] += self.weight_decay * self.weights[i]\n",
    "\n",
    "        if self.optimizer == \"sgd\":\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.lr * dw[i]\n",
    "                self.biases[i] -= self.lr * db[i]\n",
    "\n",
    "        elif self.optimizer == \"momentum\":\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.v_w[i] = self.momentum * self.v_w[i] + dw[i]\n",
    "                self.v_b[i] = self.momentum * self.v_b[i] + db[i]\n",
    "                self.weights[i] -= self.lr * self.v_w[i]\n",
    "                self.biases[i] -= self.lr * self.v_b[i]\n",
    "\n",
    "        elif self.optimizer == \"rmsprop\":\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.v_w[i] = self.beta_rms * self.v_w[i] + (1 - self.beta_rms) * (dw[i] ** 2)\n",
    "                self.v_b[i] = self.beta_rms * self.v_b[i] + (1 - self.beta_rms) * (db[i] ** 2)\n",
    "                self.weights[i] -= self.lr * dw[i] / (np.sqrt(self.v_w[i]) + self.epsilon)\n",
    "                self.biases[i]  -= self.lr * db[i] / (np.sqrt(self.v_b[i]) + self.epsilon)\n",
    "\n",
    "        elif self.optimizer == \"adam\":\n",
    "            self.t += 1\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * dw[i]\n",
    "                self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * db[i]\n",
    "                self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (dw[i] ** 2)\n",
    "                self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (db[i] ** 2)\n",
    "\n",
    "                # bias correction\n",
    "                m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "                m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "                v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "                v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "                self.weights[i] -= self.lr * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "                self.biases[i]  -= self.lr * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "        elif self.optimizer == \"nadam\":\n",
    "            self.t += 1\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * dw[i]\n",
    "                self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * db[i]\n",
    "                self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (dw[i] ** 2)\n",
    "                self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (db[i] ** 2)\n",
    "\n",
    "                # bias correction\n",
    "                m_w_hat = self.m_w[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "                m_b_hat = self.m_b[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "                v_w_hat = self.v_w[i] / (1 - self.beta2 ** (self.t + 1))\n",
    "                v_b_hat = self.v_b[i] / (1 - self.beta2 ** (self.t + 1))\n",
    "\n",
    "                grad_term_w = self.beta1 * m_w_hat + (1 - self.beta1) * dw[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "                grad_term_b = self.beta1 * m_b_hat + (1 - self.beta1) * db[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "\n",
    "                self.weights[i] -= self.lr * grad_term_w / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "                self.biases[i]  -= self.lr * grad_term_b / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "    # Training Step  with \"Nesterov\"\n",
    "    def _train_step(self, x_batch, y_batch):\n",
    "        if self.optimizer == \"nesterov\":\n",
    "            # to look-ahead: w_look = w - momentum * v\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.lr*self.momentum * self.v_w[i]\n",
    "                self.biases[i]  -= self.lr*self.momentum * self.v_b[i]\n",
    "\n",
    "            # Forward at the look-ahead position\n",
    "            self.forward_pass(x_batch)\n",
    "            out = self.h[-1]\n",
    "            l2_norm_weights = 0\n",
    "            for i in range(len(self.weights)):\n",
    "                l2_norm_weights += np.sum(self.weights[i] ** 2)\n",
    "            # for i in range(len(self.biases)):\n",
    "            #     l2_norm_bias += np.sum(self.biases[i] ** 2)\n",
    "                    \n",
    "            l2_norm_params = l2_norm_weights #+ l2_norm_bias\n",
    "            \n",
    "            loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params # (1e-10) to prevent underflow\n",
    "            #loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis=1))\n",
    "            dW, dB = self.backward_prop(y_batch)\n",
    "\n",
    "            # add weight decay here\n",
    "            for i in range(self.layer_n - 1):\n",
    "                dW[i] += self.weight_decay * self.weights[i]\n",
    "\n",
    "            # backward at the look-ahead position (go back to w_t)\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] += self.lr*self.momentum * self.v_w[i]\n",
    "                self.biases[i]  += self.lr*self.momentum * self.v_b[i]\n",
    "\n",
    "            # update velocity: u_t = momentum*u_{t-1} + dW\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.v_w[i] = self.momentum * self.v_w[i] + dW[i]\n",
    "                self.v_b[i] = self.momentum * self.v_b[i] + dB[i]\n",
    "\n",
    "            # final param update: w = w - lr*u_t\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.lr * self.v_w[i]\n",
    "                self.biases[i]  -= self.lr * self.v_b[i]\n",
    "\n",
    "            return loss\n",
    "        else:\n",
    "            # Normal forward/back\n",
    "            self.forward_pass(x_batch)\n",
    "            out = self.h[-1]\n",
    "\n",
    "            l2_norm_weights=0\n",
    "            l2_norm_bias= 0\n",
    "            for i in range(len(self.weights)):\n",
    "                l2_norm_weights += np.sum(self.weights[i] ** 2)\n",
    "            # for i in range(len(self.biases)):\n",
    "            #     l2_norm_bias += np.sum(self.biases[i] ** 2)\n",
    "                    \n",
    "            l2_norm_params = l2_norm_weights #+ l2_norm_bias\n",
    "            \n",
    "            loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params \n",
    "\n",
    "            #loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis=1))\n",
    "            dW, dB = self.backward_prop(y_batch)\n",
    "            self._update_params(dW, dB)\n",
    "            return loss\n",
    "\n",
    "    # Outer Training Loop \n",
    "    def training(self, x_train, y_train, x_val, y_val, epochs, batch_size):\n",
    "       \n",
    "        for ep in range(epochs):\n",
    "            idx = np.random.permutation(x_train.shape[0])\n",
    "            x_train_shuff = x_train[idx]\n",
    "            y_train_shuff = y_train[idx]\n",
    "            n_batches = len(x_train) // batch_size\n",
    "            epoch_loss = 0.0\n",
    "            for b in range(n_batches):\n",
    "                start = b * batch_size\n",
    "                end = start + batch_size\n",
    "                x_batch = x_train_shuff[start:end]\n",
    "                y_batch = y_train_shuff[start:end]\n",
    "                loss = self._train_step(x_batch, y_batch)\n",
    "                epoch_loss += loss\n",
    "            avg_loss = epoch_loss / n_batches\n",
    "\n",
    "            # Validation\n",
    "\n",
    "            preds = self.predict(x_val)\n",
    "            val_labels = np.argmax(y_val, axis=1)\n",
    "            val_acc = np.mean(preds == val_labels)\n",
    "\n",
    "            val_outputs = self.forward_pass(x_val)[-1]\n",
    "        \n",
    "            # Cross-entropy loss for validation\n",
    "            val_loss = -np.mean(np.sum(y_val * np.log(val_outputs + 1e-10), axis=1))\n",
    "\n",
    "            # Log metrics to wandb\n",
    "            wandb.log({\"epoch\": ep+1, \"training_loss\": avg_loss, \"validation_accuracy\": val_acc, \"validation loss\": val_loss})\n",
    "            print(f\"Epoch {ep+1}/{epochs} - loss={avg_loss:.4f}, val_acc={val_acc:.4f}, val_loss={val_loss}\" )\n",
    "\n",
    "    #Prediction \n",
    "    def predict(self, X):\n",
    "        self.forward_pass(X)\n",
    "        return np.argmax(self.h[-1], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# (x_train_full, y_train_full), (x_test, y_test) = fashion_mnist.load_data()\n",
    "# x_train_full = x_train_full.reshape(x_train_full.shape[0], -1) / 255.0\n",
    "# x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n",
    "\n",
    "# np.random.seed(42)\n",
    "# idx = np.arange(x_train_full.shape[0])\n",
    "# np.random.shuffle(idx)\n",
    "# x_train_full = x_train_full[idx]\n",
    "# y_train_full = y_train_full[idx]\n",
    "\n",
    "# # 90% training, 10% validation \n",
    "# train_size=int(.9*len(x_train_full))\n",
    "\n",
    "# x_train, y_train=x_train_full[:train_size],y_train_full[:train_size]\n",
    "# x_val, y_val=x_train_full[train_size:], y_train_full[train_size:]\n",
    "\n",
    "# num_classes = 10\n",
    "# y_train_1h = np.eye(num_classes)[y_train]\n",
    "# y_val_1h = np.eye(num_classes)[y_val]\n",
    "# y_test_1h = np.eye(num_classes)[y_test]\n",
    "\n",
    "# # model\n",
    "# model = feed_forward_NN_4(\n",
    "#     layers=[784] + [32] *3 + [10],\n",
    "# optimizer=\"nesterov\",\n",
    "# learning_rate=0.01,\n",
    "# momentum=0.9,\n",
    "# beta1=0.9,\n",
    "# beta2=0.999,\n",
    "# beta_rms=0.9,\n",
    "# epsilon=1e-4,\n",
    "# weight_decay=0.0005,\n",
    "# init_type=\"xavier\",\n",
    "# activation=\"relu\")\n",
    "\n",
    "#     # Train the model\n",
    "# model.training(\n",
    "#         x_train=x_train,\n",
    "#         y_train=y_train_1h,\n",
    "#         x_val=x_val,\n",
    "#         y_val=y_val_1h,\n",
    "#         epochs=10,\n",
    "#         batch_size=32\n",
    "#     )\n",
    "\n",
    "#     #Evaluation on test set\n",
    "# test_preds = model.predict(x_test)\n",
    "# test_labels = np.argmax(y_test_1h, axis=1)\n",
    "# test_acc = np.mean(test_preds == test_labels)\n",
    "# print(\"test accuracy \",test_acc)\n",
    "# #wandb.log({\"test_accuracy\": test_acc})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# train_sweep() function\n",
    "\n",
    "def train_sweep():\n",
    "    # Initialize wandb\n",
    "    wandb.init()\n",
    "    config = wandb.config\n",
    "\n",
    "    #custom run name from hyperparameters\n",
    "    run_name = f\"hl_{config.num_hidden_layers}_bs_{config.batch_size}_ac_{config.activation}_opt_{config.optimizer}\"\n",
    "    wandb.run.name = run_name\n",
    "\n",
    "    # Load Fashion-MNIST\n",
    "    (x_train_full, y_train_full), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    x_train_full = x_train_full.reshape(x_train_full.shape[0], -1) / 255.0\n",
    "    x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n",
    "\n",
    "    np.random.seed(42)\n",
    "    idx = np.arange(x_train_full.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    x_train_full = x_train_full[idx]\n",
    "    y_train_full = y_train_full[idx]\n",
    "\n",
    "    # 90% training, 10% validation \n",
    "    train_size=int(.9*len(x_train_full))\n",
    "\n",
    "    x_train, y_train=x_train_full[:train_size],y_train_full[:train_size]\n",
    "    x_val, y_val=x_train_full[train_size:], y_train_full[train_size:]\n",
    "\n",
    "    num_classes = 10\n",
    "    y_train_1h = np.eye(num_classes)[y_train]\n",
    "    y_val_1h = np.eye(num_classes)[y_val]\n",
    "    y_test_1h = np.eye(num_classes)[y_test]\n",
    "\n",
    "    # model\n",
    "    model = feed_forward_NN_4(\n",
    "        layers=[784] + [config.hidden_size] * config.num_hidden_layers + [10],\n",
    "        optimizer=config.optimizer,\n",
    "        learning_rate=config.learning_rate,\n",
    "        momentum=config.momentum,\n",
    "        beta1=config.beta1,\n",
    "        beta2=config.beta2,\n",
    "        beta_rms=config.beta_rms,\n",
    "        epsilon=config.epsilon,\n",
    "        weight_decay=config.weight_decay,\n",
    "        init_type=config.init_type,\n",
    "        activation=config.activation\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.training(\n",
    "        x_train=x_train,\n",
    "        y_train=y_train_1h,\n",
    "        x_val=x_val,\n",
    "        y_val=y_val_1h,\n",
    "        epochs=config.epochs,\n",
    "        batch_size=config.batch_size\n",
    "    )\n",
    "\n",
    "    #Evaluation on test set\n",
    "    test_preds = model.predict(x_test)\n",
    "    test_labels = np.argmax(y_test_1h, axis=1)\n",
    "    test_acc = np.mean(test_preds == test_labels)\n",
    "    \n",
    "    wandb.log({\"test_accuracy\": test_acc})\n",
    "    print(\"test accuracy \",test_acc)\n",
    "\n",
    "\n",
    "# sweep configuration\n",
    "sweep_config = {\n",
    "    \"method\": \"random\", \n",
    "    \"metric\": {\n",
    "        \"name\": \"validation_accuracy\",\n",
    "        \"goal\": \"maximize\"\n",
    "    },\n",
    "    \"parameters\": {\n",
    "        \"epochs\": {\"values\": [5,10]},\n",
    "        \"num_hidden_layers\": {\"values\": [3, 4, 5]},\n",
    "        \"hidden_size\": {\"values\": [32,64,128]},\n",
    "        \"weight_decay\": {\"values\": [0.0, 5e-4, 0.5]},\n",
    "        \"learning_rate\": {\"values\": [1e-3, 1e-4]},\n",
    "        \"optimizer\": {\"values\": [\"sgd\", \"momentum\", \"nesterov\", \"rmsprop\", \"adam\", \"nadam\"]},\n",
    "        \"batch_size\": {\"values\": [16, 32, 64]},\n",
    "        \"init_type\": {\"values\": [\"random\", \"xavier\"]},\n",
    "        \"activation\": {\"values\": [\"sigmoid\", \"tanh\", \"relu\"]},\n",
    "        \"momentum\": {\"values\": [0.9]},\n",
    "        \"beta1\": {\"values\": [0.9]},\n",
    "        \"beta2\": {\"values\": [0.999,0.99]},\n",
    "        \"beta_rms\": {\"values\": [0.9]},\n",
    "        \"epsilon\": {\"values\": [1e-8]},\n",
    "        \"loss_func\":{\"values\":[\"cross_entropy\"]}\n",
    "    }\n",
    "}\n",
    "\n",
    "# Running the sweep\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Creating sweep\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"q4_sweep_project\")\n",
    "    # Launching sweep agent\n",
    "    wandb.agent(sweep_id, function=train_sweep)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
