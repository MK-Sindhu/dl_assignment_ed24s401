{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "CommError",
     "evalue": "failed to upsert bucket: returned error 403: {\"data\":{\"upsertBucket\":null},\"errors\":[{\"message\":\"permission denied\",\"path\":[\"upsertBucket\"],\"extensions\":{\"code\":\"PERMISSION_ERROR\"}}]}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCommError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 390\u001b[39m\n\u001b[32m    386\u001b[39m args, unknown = parser.parse_known_args()\n\u001b[32m    388\u001b[39m config = \u001b[38;5;28mvars\u001b[39m(args)\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m \u001b[43mwandb\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwandb_project\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mentity\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwandb_entity\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[33m'\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m'\u001b[39m] ==\u001b[33m\"\u001b[39m\u001b[33mfashion_mnist\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    397\u001b[39m \u001b[38;5;66;03m# Load Fashion-MNIST\u001b[39;00m\n\u001b[32m    398\u001b[39m     (x_train_full, y_train_full), (x_test, y_test) = fashion_mnist.load_data()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py:1482\u001b[39m, in \u001b[36minit\u001b[39m\u001b[34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[39m\n\u001b[32m   1478\u001b[39m     wl._get_logger().exception(\u001b[33m\"\u001b[39m\u001b[33merror in wandb.init()\u001b[39m\u001b[33m\"\u001b[39m, exc_info=e)\n\u001b[32m   1480\u001b[39m \u001b[38;5;66;03m# Need to build delay into this sentry capture because our exit hooks\u001b[39;00m\n\u001b[32m   1481\u001b[39m \u001b[38;5;66;03m# mess with sentry's ability to send out errors before the program ends.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1482\u001b[39m \u001b[43mwandb\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sentry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1483\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wandb\\analytics\\sentry.py:156\u001b[39m, in \u001b[36mSentry.reraise\u001b[39m\u001b[34m(self, exc)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28mself\u001b[39m.exception(exc)\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# this will messily add this \"reraise\" function to the stack trace,\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# but hopefully it's not too bad\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc.with_traceback(sys.exc_info()[\u001b[32m2\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py:1468\u001b[39m, in \u001b[36minit\u001b[39m\u001b[34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, anonymous, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings)\u001b[39m\n\u001b[32m   1465\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m run_settings.x_server_side_derived_summary:\n\u001b[32m   1466\u001b[39m         init_telemetry.feature.server_side_derived_summary = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1468\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwi\u001b[49m\u001b[43m.\u001b[49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_settings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_config\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1470\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1471\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wl:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\acer\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\wandb\\sdk\\wandb_init.py:963\u001b[39m, in \u001b[36m_WandbInit.init\u001b[39m\u001b[34m(self, settings, config)\u001b[39m\n\u001b[32m    960\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m result.run_result\n\u001b[32m    962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error := ProtobufErrorHandler.to_exception(result.run_result.error):\n\u001b[32m--> \u001b[39m\u001b[32m963\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error\n\u001b[32m    965\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result.run_result.HasField(\u001b[33m\"\u001b[39m\u001b[33mrun\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    966\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Error(\u001b[33m\"\u001b[39m\u001b[33mAssertion failed: run_result is missing the run field\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mCommError\u001b[39m: failed to upsert bucket: returned error 403: {\"data\":{\"upsertBucket\":null},\"errors\":[{\"message\":\"permission denied\",\"path\":[\"upsertBucket\"],\"extensions\":{\"code\":\"PERMISSION_ERROR\"}}]}"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.datasets import fashion_mnist\n",
    "import wandb\n",
    "import argparse\n",
    "\n",
    "\n",
    "# Neural Network Class: feed_forward_NN_4\n",
    "\n",
    "class feed_forward_NN_final:\n",
    "    def __init__(self,\n",
    "                 layers,\n",
    "                 optimizer,\n",
    "                 learning_rate,\n",
    "                 momentum,\n",
    "                 beta1,\n",
    "                 beta2,\n",
    "                 beta,\n",
    "                 epsilon,\n",
    "                 weight_decay,\n",
    "                 weight_init,\n",
    "                 activation,\n",
    "                 loss\n",
    "                 ):\n",
    "    \n",
    "        \n",
    "        self.layers = layers\n",
    "        self.layer_n = len(layers)\n",
    "        self.optimizer = optimizer.lower()\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.beta_rms = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.weight_decay = weight_decay\n",
    "        self.weight_init = weight_init.lower()\n",
    "        self.activation = activation.lower()\n",
    "        self.loss=loss.lower()\n",
    "\n",
    "        # Initialize Weights & BiaseS\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.layer_n - 1):\n",
    "            if self.weight_init == \"xavier\":\n",
    "                # \"Xavier\" initialization\n",
    "                w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(1.0 / layers[i])\n",
    "            else:\n",
    "                # \"random\" initialization\n",
    "                w = np.random.randn(layers[i], layers[i+1]) * np.sqrt(2.0 / layers[i])\n",
    "            b = np.zeros((1, layers[i+1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "        # initialize extra Params \n",
    "        if self.optimizer in [\"momentum\", \"nag\", \"rmsprop\", \"adam\", \"nadam\"]:\n",
    "            self.v_w = [np.zeros_like(w) for w in self.weights]\n",
    "            self.v_b = [np.zeros_like(b) for b in self.biases]\n",
    "        if self.optimizer in [\"adam\", \"nadam\"]:\n",
    "            self.m_w = [np.zeros_like(w) for w in self.weights]\n",
    "            self.m_b = [np.zeros_like(b) for b in self.biases]\n",
    "            self.t = 0\n",
    "\n",
    "    # activations \n",
    "    def sigmoid(self, x):\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def activate(self, x):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return self.sigmoid(x)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return self.tanh(x)\n",
    "        elif self.activation == \"relu\":\n",
    "            return self.relu(x)\n",
    "        else:\n",
    "            return self.sigmoid(x) \n",
    "        \n",
    "    # derivatives\n",
    "    def derivative(self, a):\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return a * (1 - a)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return 1 - a**2\n",
    "        elif self.activation == \"relu\":\n",
    "            return (a > 0).astype(float)\n",
    "        else:\n",
    "            return a * (1 - a) \n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    # Forward Pass\n",
    "    def forward_pass(self, x):\n",
    "        self.h = [x]  \n",
    "        # Hidden layers\n",
    "        for i in range(self.layer_n - 2):\n",
    "            z = np.dot(self.h[i], self.weights[i]) + self.biases[i]\n",
    "            act = self.activate(z)\n",
    "            self.h.append(act)\n",
    "        # Output layer- softmax\n",
    "        z_out = np.dot(self.h[-1], self.weights[-1]) + self.biases[-1]\n",
    "        out = self.softmax(z_out)\n",
    "        self.h.append(out)\n",
    "        return self.h\n",
    "\n",
    "    # Backward Pass\n",
    "    def backward_prop(self, y_true):\n",
    "        m = y_true.shape[0]\n",
    "        dw = [None] * (self.layer_n - 1)\n",
    "        db = [None] * (self.layer_n - 1)\n",
    "\n",
    "        # Cross-entropy derivative for output layer\n",
    "        if self.loss==\"cross_entropy\":\n",
    "            delta = self.h[-1] - y_true  # shape: (batch_size, output_dim)\n",
    "        elif self.loss==\"mean_squared_error\":\n",
    "            batch_size_sq=len(self.h[-1])\n",
    "            classes_sq=len(self.h[-1][0])\n",
    "            delta=np.zeros((batch_size_sq,classes_sq))\n",
    "\n",
    "            for i in range(batch_size_sq):\n",
    "                jacobian_softmax= np.diag(self.h[-1][i]) - np.outer(self.h[-1][i], self.h[-1][i])\n",
    "                # print(jacobian_softmax.shape)\n",
    "                # print(self.h[-1][i])    \n",
    "                delta[i]= 2*np.dot(self.h[-1][i]-y_true[i], jacobian_softmax)\n",
    "                \n",
    "        else:\n",
    "            delta = self.h[-1] - y_true \n",
    "\n",
    "\n",
    "        # Propagation\n",
    "        for i in reversed(range(self.layer_n - 1)):\n",
    "            dw[i] = np.dot(self.h[i].T, delta) / m\n",
    "            db[i] = np.sum(delta, axis=0, keepdims=True) / m\n",
    "            if i > 0:\n",
    "                # For hidden layers, multiply by derivative of activation\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.derivative(self.h[i])\n",
    "        return dw, db\n",
    "\n",
    "    # Param Updates for \"Non-Nesterov\" \n",
    "    def _update_params(self, dw, db):\n",
    "        # Add weight decay to each gradient\n",
    "        for i in range(self.layer_n - 1):\n",
    "            dw[i] += self.weight_decay * self.weights[i]\n",
    "\n",
    "        if self.optimizer == \"sgd\":\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.lr * dw[i]\n",
    "                self.biases[i] -= self.lr * db[i]\n",
    "\n",
    "        elif self.optimizer == \"momentum\":\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.v_w[i] = self.momentum * self.v_w[i] + dw[i]\n",
    "                self.v_b[i] = self.momentum * self.v_b[i] + db[i]\n",
    "                self.weights[i] -= self.lr * self.v_w[i]\n",
    "                self.biases[i] -= self.lr * self.v_b[i]\n",
    "\n",
    "        elif self.optimizer == \"rmsprop\":\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.v_w[i] = self.beta * self.v_w[i] + (1 - self.beta) * (dw[i] ** 2)\n",
    "                self.v_b[i] = self.beta * self.v_b[i] + (1 - self.beta) * (db[i] ** 2)\n",
    "                self.weights[i] -= self.lr * dw[i] / (np.sqrt(self.v_w[i]) + self.epsilon)\n",
    "                self.biases[i]  -= self.lr * db[i] / (np.sqrt(self.v_b[i]) + self.epsilon)\n",
    "\n",
    "        elif self.optimizer == \"adam\":\n",
    "            self.t += 1\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * dw[i]\n",
    "                self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * db[i]\n",
    "                self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (dw[i] ** 2)\n",
    "                self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (db[i] ** 2)\n",
    "\n",
    "                # bias correction\n",
    "                m_w_hat = self.m_w[i] / (1 - self.beta1 ** self.t)\n",
    "                m_b_hat = self.m_b[i] / (1 - self.beta1 ** self.t)\n",
    "                v_w_hat = self.v_w[i] / (1 - self.beta2 ** self.t)\n",
    "                v_b_hat = self.v_b[i] / (1 - self.beta2 ** self.t)\n",
    "\n",
    "                self.weights[i] -= self.lr * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "                self.biases[i]  -= self.lr * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "        elif self.optimizer == \"nadam\":\n",
    "            self.t += 1\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * dw[i]\n",
    "                self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * db[i]\n",
    "                self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (dw[i] ** 2)\n",
    "                self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (db[i] ** 2)\n",
    "\n",
    "                # bias correction\n",
    "                m_w_hat = self.m_w[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "                m_b_hat = self.m_b[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "                v_w_hat = self.v_w[i] / (1 - self.beta2 ** (self.t + 1))\n",
    "                v_b_hat = self.v_b[i] / (1 - self.beta2 ** (self.t + 1))\n",
    "\n",
    "                grad_term_w = self.beta1 * m_w_hat + (1 - self.beta1) * dw[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "                grad_term_b = self.beta1 * m_b_hat + (1 - self.beta1) * db[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "\n",
    "                self.weights[i] -= self.lr * grad_term_w / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "                self.biases[i]  -= self.lr * grad_term_b / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "    # Training Step  with \"Nesterov\"\n",
    "    def _train_step(self, x_batch, y_batch):\n",
    "        if self.optimizer == \"nag\":\n",
    "            # to look-ahead: w_look = w - momentum * v\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.lr*self.momentum * self.v_w[i]\n",
    "                self.biases[i]  -= self.lr*self.momentum * self.v_b[i]\n",
    "\n",
    "            # Forward at the look-ahead position\n",
    "            self.forward_pass(x_batch)\n",
    "            out = self.h[-1]\n",
    "            l2_norm_weights = 0\n",
    "            for i in range(len(self.weights)):\n",
    "                l2_norm_weights += np.sum(self.weights[i] ** 2)\n",
    "            # for i in range(len(self.biases)):\n",
    "            #     l2_norm_bias += np.sum(self.biases[i] ** 2)\n",
    "                    \n",
    "            l2_norm_params = l2_norm_weights #+ l2_norm_bias\n",
    "\n",
    "            if self.loss==\"cross_entropy\":\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params # (1e-10) to prevent underflow\n",
    "            elif self.loss==\"mean_squared_error\" :\n",
    "                loss= 0.5 * np.mean(np.sum((out - y_batch)**2, axis=1))\n",
    "            else:\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params\n",
    "            \n",
    "            \n",
    "            #loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params # (1e-10) to prevent underflow\n",
    "            #loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis=1))\n",
    "            dW, dB = self.backward_prop(y_batch)\n",
    "\n",
    "            # add weight decay here\n",
    "            for i in range(self.layer_n - 1):\n",
    "                dW[i] += self.weight_decay * self.weights[i]\n",
    "\n",
    "            # backward at the look-ahead position (go back to w_t)\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] += self.lr*self.momentum * self.v_w[i]\n",
    "                self.biases[i]  += self.lr*self.momentum * self.v_b[i]\n",
    "\n",
    "            # update velocity: u_t = momentum*u_{t-1} + dW\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.v_w[i] = self.momentum * self.v_w[i] + dW[i]\n",
    "                self.v_b[i] = self.momentum * self.v_b[i] + dB[i]\n",
    "\n",
    "            # final param update: w = w - lr*u_t\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.lr * self.v_w[i]\n",
    "                self.biases[i]  -= self.lr * self.v_b[i]\n",
    "\n",
    "            return loss\n",
    "        else:\n",
    "            # Normal forward/back\n",
    "            self.forward_pass(x_batch)\n",
    "            out = self.h[-1]\n",
    "\n",
    "            l2_norm_weights=0\n",
    "            \n",
    "            for i in range(len(self.weights)):\n",
    "                l2_norm_weights += np.sum(self.weights[i] ** 2)\n",
    "            # for i in range(len(self.biases)):\n",
    "            #     l2_norm_bias += np.sum(self.biases[i] ** 2)\n",
    "                    \n",
    "            l2_norm_params = l2_norm_weights #+ l2_norm_bias\n",
    "\n",
    "            if self.loss==\"cross_entropy\":\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params # (1e-10) to prevent underflow\n",
    "            elif self.loss==\"mean_squared_error\" :\n",
    "                loss= 0.5 * np.mean(np.sum((out - y_batch)**2, axis=1))\n",
    "            else:\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params    \n",
    "            \n",
    "            \n",
    "             \n",
    "\n",
    "            #loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis=1))\n",
    "            dW, dB = self.backward_prop(y_batch)\n",
    "            self._update_params(dW, dB)\n",
    "            return loss\n",
    "\n",
    "    # Outer Training Loop \n",
    "    def training(self, x_train, y_train, x_val, y_val, epochs, batch_size):\n",
    "       \n",
    "        for ep in range(epochs):\n",
    "            idx = np.random.permutation(x_train.shape[0])\n",
    "            x_train_shuff = x_train[idx]\n",
    "            y_train_shuff = y_train[idx]\n",
    "            n_batches = len(x_train) // batch_size\n",
    "            epoch_loss = 0.0\n",
    "            for b in range(n_batches):\n",
    "                start = b * batch_size\n",
    "                end = start + batch_size\n",
    "                x_batch = x_train_shuff[start:end]\n",
    "                y_batch = y_train_shuff[start:end]\n",
    "                loss = self._train_step(x_batch, y_batch)\n",
    "                epoch_loss += loss\n",
    "            avg_loss = epoch_loss / n_batches\n",
    "\n",
    "            # Validation\n",
    "\n",
    "            preds = self.predict(x_val)\n",
    "            val_labels = np.argmax(y_val, axis=1)\n",
    "            val_acc = np.mean(preds == val_labels)\n",
    "\n",
    "            val_outputs = self.forward_pass(x_val)[-1]\n",
    "        \n",
    "            # Cross-entropy loss for validation\n",
    "            #val_loss = -np.mean(np.sum(y_val * np.log(val_outputs + 1e-10), axis=1))\n",
    "\n",
    "            l2_norm_weights=0\n",
    "            \n",
    "            for i in range(len(self.weights)):\n",
    "                l2_norm_weights += np.sum(self.weights[i] ** 2)\n",
    "            # for i in range(len(self.biases)):\n",
    "            #     l2_norm_bias += np.sum(self.biases[i] ** 2)\n",
    "                    \n",
    "            l2_norm_params = l2_norm_weights\n",
    "\n",
    "            if self.loss==\"cross_entropy\":\n",
    "                val_loss = -np.mean(np.sum(y_val * np.log(val_outputs + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params # (1e-10) to prevent underflow\n",
    "            elif self.loss==\"mean_squared_error\" :\n",
    "                val_loss= 0.5 * np.mean(np.sum((val_outputs - y_val)**2, axis=1))\n",
    "            else:\n",
    "                val_loss = -np.mean(np.sum(y_val * np.log(val_outputs + 1e-10), axis = 1)) +  (self.weight_decay/2) * l2_norm_params\n",
    "\n",
    "            # Log metrics to wandb\n",
    "            wandb.log({\"epoch\": ep+1, \"training_loss\": avg_loss, \"validation_accuracy\": val_acc, \"validation loss\": val_loss})\n",
    "            print(f\"Epoch {ep+1}/{epochs} - loss={avg_loss:.4f}, val_acc={val_acc:.4f}, val_loss={val_loss}\" )\n",
    "\n",
    "    #Prediction \n",
    "    def predict(self, X):\n",
    "        self.forward_pass(X)\n",
    "        return np.argmax(self.h[-1], axis=1)\n",
    "\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parser = argparse.ArgumentParser(description='Train Neural Network with backpropagation.')\n",
    "    \n",
    "    parser.add_argument('-wp', '--wandb_project', type=str, default=\"myprojectname\", help='Project name used to track experiments in Weights & Biases dashboard')\n",
    "    \n",
    "    parser.add_argument('-we', '--wandb_entity', type=str, default='myname', help='Wandb Entity used to track experiments in the Weights & Biases dashboard')\n",
    "    \n",
    "    parser.add_argument('-d', '--dataset', type=str, default='fashion_mnist', choices=['mnist', 'fashion_mnist'], help='Dataset to use')\n",
    "    \n",
    "    parser.add_argument('-e', '--epochs', type=int, default=1, help='Number of epochs to train neural network')\n",
    "    \n",
    "    parser.add_argument('-b', '--batch_size', type=int, default=4, help='Batch size used to train neural network')\n",
    "    \n",
    "    parser.add_argument('-l', '--loss', type=str, default='cross_entropy', choices=['mean_squared_error', 'cross_entropy'], help='Loss function')\n",
    "    \n",
    "    parser.add_argument('-o', '--optimizer', type=str, default='sgd', choices=['sgd', 'momentum', 'nag', 'rmsprop', 'adam', 'nadam'], help='Optimizer type')\n",
    "    \n",
    "    parser.add_argument('-lr', '--learning_rate', type=float, default=0.1, help='Learning rate used to optimize model parameters')\n",
    "    \n",
    "    parser.add_argument('-m', '--momentum', type=float, default=0.5, help='Momentum used by momentum and nag optimizers')\n",
    "    \n",
    "    parser.add_argument('-beta', '--beta', type=float, default=0.5, help='Beta used by rmsprop optimizer')\n",
    "    \n",
    "    parser.add_argument('-beta1', '--beta1', type=float, default=0.5, help='Beta1 used by adam and nadam optimizers')\n",
    "    \n",
    "    parser.add_argument('-beta2', '--beta2', type=float, default=0.5, help='Beta2 used by adam and nadam optimizers')\n",
    "    \n",
    "    parser.add_argument('-eps', '--epsilon', type=float, default=0.000001, help='Epsilon used by optimizers')\n",
    "    \n",
    "    parser.add_argument('-w_d', '--weight_decay', type=float, default=0.0, help='Weight decay used by optimizers')\n",
    "    \n",
    "    parser.add_argument('-w_i', '--weight_init', type=str, default='random', choices=['random', 'Xavier'], help='Weight initialization method')\n",
    "    \n",
    "    parser.add_argument('-nhl', '--num_layers', type=int, default=1, help='Number of hidden layers used in feedforward neural network')\n",
    "    \n",
    "    parser.add_argument('-sz', '--hidden_size', type=int, default=4, help='Number of hidden neurons in a feedforward layer')\n",
    "    \n",
    "    parser.add_argument('-a', '--activation', type=str, default='sigmoid', choices=['identity', 'sigmoid', 'tanh', 'ReLU'], help='Activation function')\n",
    "\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    \n",
    "    config = vars(args)\n",
    "\n",
    "    wandb.init(project=config['wandb_project'], entity=config['wandb_entity'], config=config)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    if config['dataset'] ==\"fashion_mnist\":\n",
    "    # Load Fashion-MNIST\n",
    "        (x_train_full, y_train_full), (x_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "    elif config['dataset']==\"mnist\":\n",
    "        (x_train_full, y_train_full), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset: {config.dataset}\")\n",
    "\n",
    "    x_train_full = x_train_full.reshape(x_train_full.shape[0], -1) / 255.0\n",
    "    x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n",
    "\n",
    "    np.random.seed(42)\n",
    "    idx = np.arange(x_train_full.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    x_train_full = x_train_full[idx]\n",
    "    y_train_full = y_train_full[idx]\n",
    "\n",
    "    # 90% training, 10% validation \n",
    "    train_size=int(.9*len(x_train_full))\n",
    "\n",
    "    x_train, y_train=x_train_full[:train_size],y_train_full[:train_size]\n",
    "    x_val, y_val=x_train_full[train_size:], y_train_full[train_size:]\n",
    "\n",
    "    num_classes = 10\n",
    "    y_train_1h = np.eye(num_classes)[y_train]\n",
    "    y_val_1h = np.eye(num_classes)[y_val]\n",
    "    y_test_1h = np.eye(num_classes)[y_test]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #model\n",
    "    model = feed_forward_NN_final(\n",
    "    layers=[784] + [config['hidden_size']] * config['num_hidden_layers'] + [10],\n",
    "    optimizer=config['optimizer'],\n",
    "    learning_rate=config['learning_rate'],\n",
    "    momentum=config['momentum'],\n",
    "    beta1=config['beta1'],\n",
    "    beta2=config['beta2'],\n",
    "    beta=config['beta'],\n",
    "    epsilon=config['epsilon'],\n",
    "    weight_decay=config['weight_decay'],\n",
    "    init_type=config['weight_init'],\n",
    "    activation=config['activation'],\n",
    "    loss_func=config['loss']\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    model.training(\n",
    "    x_train=x_train,\n",
    "    y_train=y_train_1h,\n",
    "    x_val=x_val,\n",
    "    y_val=y_val_1h,\n",
    "    epochs=config['epochs'],\n",
    "    batch_size=config['batch_size']\n",
    "    )\n",
    "\n",
    "    #Evaluation on test set\n",
    "    test_preds = model.predict(x_test)\n",
    "    test_labels = np.argmax(y_test_1h, axis=1)\n",
    "    test_acc = np.mean(test_preds == test_labels)\n",
    "\n",
    "    wandb.log({\"test_accuracy\": test_acc})\n",
    "    print(\"test accuracy \",test_acc)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 - loss=0.7907, val_acc=0.6962, val_loss=0.8374731050382146\n",
      "Test accuracy: 0.6956\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.datasets import fashion_mnist\n",
    "import wandb\n",
    "import argparse\n",
    "\n",
    "\n",
    "# Neural Network Class: feed_forward_NN_final\n",
    "class feed_forward_NN_final:\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers,\n",
    "        optimizer,\n",
    "        learning_rate,\n",
    "        momentum,\n",
    "        beta1,\n",
    "        beta2,\n",
    "        beta,\n",
    "        epsilon,\n",
    "        weight_decay,\n",
    "        weight_init,\n",
    "        activation,\n",
    "        loss\n",
    "    ):\n",
    "        self.layers = layers\n",
    "        self.layer_n = len(layers)\n",
    "        self.optimizer = optimizer.lower()\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.beta_rms = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.weight_decay = weight_decay\n",
    "        self.weight_init = weight_init.lower()\n",
    "        self.activation = activation.lower()\n",
    "        self.loss = loss.lower()\n",
    "\n",
    "        # Initialize Weights & Biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.layer_n - 1):\n",
    "            if self.weight_init == \"xavier\":\n",
    "                # \"Xavier\" initialization\n",
    "                w = np.random.randn(layers[i], layers[i + 1]) * np.sqrt(1.0 / layers[i])\n",
    "            else:\n",
    "                # \"random\" initialization\n",
    "                w = np.random.randn(layers[i], layers[i + 1]) * np.sqrt(2.0 / layers[i])\n",
    "            b = np.zeros((1, layers[i + 1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "        # Initialize extra params if needed\n",
    "        if self.optimizer in [\"momentum\", \"nag\", \"rmsprop\", \"adam\", \"nadam\"]:\n",
    "            self.v_w = [np.zeros_like(w) for w in self.weights]\n",
    "            self.v_b = [np.zeros_like(b) for b in self.biases]\n",
    "        if self.optimizer in [\"adam\", \"nadam\"]:\n",
    "            self.m_w = [np.zeros_like(w) for w in self.weights]\n",
    "            self.m_b = [np.zeros_like(b) for b in self.biases]\n",
    "            self.t = 0\n",
    "\n",
    "    # --- Activation Functions ---\n",
    "    def sigmoid(self, x):\n",
    "        # Clip to avoid overflow in exp()\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def activate(self, x):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return self.sigmoid(x)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return self.tanh(x)\n",
    "        elif self.activation == \"relu\":\n",
    "            return self.relu(x)\n",
    "        else:\n",
    "            # default\n",
    "            return self.sigmoid(x)\n",
    "\n",
    "    # Derivatives of Activation\n",
    "    def derivative(self, a):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return a * (1 - a)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return 1 - a**2\n",
    "        elif self.activation == \"relu\":\n",
    "            return (a > 0).astype(float)\n",
    "        else:\n",
    "            return a * (1 - a)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    # --- Forward Pass ---\n",
    "    def forward_pass(self, x):\n",
    "        self.h = [x]\n",
    "        # Hidden layers\n",
    "        for i in range(self.layer_n - 2):\n",
    "            z = np.dot(self.h[i], self.weights[i]) + self.biases[i]\n",
    "            act = self.activate(z)\n",
    "            self.h.append(act)\n",
    "        # Output layer: softmax\n",
    "        z_out = np.dot(self.h[-1], self.weights[-1]) + self.biases[-1]\n",
    "        out = self.softmax(z_out)\n",
    "        self.h.append(out)\n",
    "        return self.h\n",
    "\n",
    "    # --- Backward Pass ---\n",
    "    def backward_prop(self, y_true):\n",
    "        m = y_true.shape[0]\n",
    "        dw = [None] * (self.layer_n - 1)\n",
    "        db = [None] * (self.layer_n - 1)\n",
    "\n",
    "        # Output layer delta\n",
    "        if self.loss == \"cross_entropy\":\n",
    "            # Cross-entropy derivative\n",
    "            delta = self.h[-1] - y_true\n",
    "        elif self.loss == \"mean_squared_error\":\n",
    "            # MSE derivative wrt softmax\n",
    "            batch_size_sq = len(self.h[-1])\n",
    "            classes_sq = len(self.h[-1][0])\n",
    "            delta = np.zeros((batch_size_sq, classes_sq))\n",
    "            for i in range(batch_size_sq):\n",
    "                jacobian_softmax = (\n",
    "                    np.diag(self.h[-1][i]) - np.outer(self.h[-1][i], self.h[-1][i])\n",
    "                )\n",
    "                delta[i] = 2 * np.dot(self.h[-1][i] - y_true[i], jacobian_softmax)\n",
    "        else:\n",
    "            # default to cross-entropy style\n",
    "            delta = self.h[-1] - y_true\n",
    "\n",
    "        # Backprop through layers\n",
    "        for i in reversed(range(self.layer_n - 1)):\n",
    "            dw[i] = np.dot(self.h[i].T, delta) / m\n",
    "            db[i] = np.sum(delta, axis=0, keepdims=True) / m\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.derivative(self.h[i])\n",
    "        return dw, db\n",
    "\n",
    "    # --- Parameter Updates (Non-Nesterov) ---\n",
    "    def _update_params(self, dw, db):\n",
    "        # Add weight decay\n",
    "        for i in range(self.layer_n - 1):\n",
    "            dw[i] += self.weight_decay * self.weights[i]\n",
    "\n",
    "        if self.optimizer == \"sgd\":\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.lr * dw[i]\n",
    "                self.biases[i] -= self.lr * db[i]\n",
    "\n",
    "        elif self.optimizer == \"momentum\":\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.v_w[i] = self.momentum * self.v_w[i] + dw[i]\n",
    "                self.v_b[i] = self.momentum * self.v_b[i] + db[i]\n",
    "                self.weights[i] -= self.lr * self.v_w[i]\n",
    "                self.biases[i] -= self.lr * self.v_b[i]\n",
    "\n",
    "        elif self.optimizer == \"rmsprop\":\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.v_w[i] = self.beta_rms * self.v_w[i] + (1 - self.beta_rms) * (\n",
    "                    dw[i] ** 2\n",
    "                )\n",
    "                self.v_b[i] = self.beta_rms * self.v_b[i] + (1 - self.beta_rms) * (\n",
    "                    db[i] ** 2\n",
    "                )\n",
    "                self.weights[i] -= self.lr * dw[i] / (np.sqrt(self.v_w[i]) + self.epsilon)\n",
    "                self.biases[i] -= self.lr * db[i] / (np.sqrt(self.v_b[i]) + self.epsilon)\n",
    "\n",
    "        elif self.optimizer == \"adam\":\n",
    "            self.t += 1\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * dw[i]\n",
    "                self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * db[i]\n",
    "                self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (dw[i] ** 2)\n",
    "                self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (db[i] ** 2)\n",
    "\n",
    "                # Bias correction\n",
    "                m_w_hat = self.m_w[i] / (1 - self.beta1**self.t)\n",
    "                m_b_hat = self.m_b[i] / (1 - self.beta1**self.t)\n",
    "                v_w_hat = self.v_w[i] / (1 - self.beta2**self.t)\n",
    "                v_b_hat = self.v_b[i] / (1 - self.beta2**self.t)\n",
    "\n",
    "                self.weights[i] -= self.lr * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "                self.biases[i] -= self.lr * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "        elif self.optimizer == \"nadam\":\n",
    "            self.t += 1\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * dw[i]\n",
    "                self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * db[i]\n",
    "                self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (dw[i] ** 2)\n",
    "                self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (db[i] ** 2)\n",
    "\n",
    "                # Bias correction\n",
    "                m_w_hat = self.m_w[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "                m_b_hat = self.m_b[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "                v_w_hat = self.v_w[i] / (1 - self.beta2 ** (self.t + 1))\n",
    "                v_b_hat = self.v_b[i] / (1 - self.beta2 ** (self.t + 1))\n",
    "\n",
    "                grad_term_w = self.beta1 * m_w_hat + (1 - self.beta1) * dw[i] / (\n",
    "                    1 - self.beta1 ** (self.t + 1)\n",
    "                )\n",
    "                grad_term_b = self.beta1 * m_b_hat + (1 - self.beta1) * db[i] / (\n",
    "                    1 - self.beta1 ** (self.t + 1)\n",
    "                )\n",
    "\n",
    "                self.weights[i] -= self.lr * grad_term_w / (\n",
    "                    np.sqrt(v_w_hat) + self.epsilon\n",
    "                )\n",
    "                self.biases[i] -= self.lr * grad_term_b / (\n",
    "                    np.sqrt(v_b_hat) + self.epsilon\n",
    "                )\n",
    "\n",
    "    # --- Training Step (Includes Nesterov) ---\n",
    "    def _train_step(self, x_batch, y_batch):\n",
    "        if self.optimizer == \"nag\":\n",
    "            # Look-ahead\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.lr * self.momentum * self.v_w[i]\n",
    "                self.biases[i] -= self.lr * self.momentum * self.v_b[i]\n",
    "\n",
    "            self.forward_pass(x_batch)\n",
    "            out = self.h[-1]\n",
    "\n",
    "            # L2 norm (for weight decay)\n",
    "            l2_norm_weights = sum(np.sum(w**2) for w in self.weights)\n",
    "            l2_norm_params = l2_norm_weights\n",
    "\n",
    "            # Loss\n",
    "            if self.loss == \"cross_entropy\":\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis=1)) + (\n",
    "                    self.weight_decay / 2\n",
    "                ) * l2_norm_params\n",
    "            elif self.loss == \"mean_squared_error\":\n",
    "                loss = 0.5 * np.mean(np.sum((out - y_batch) ** 2, axis=1))\n",
    "            else:\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis=1)) + (\n",
    "                    self.weight_decay / 2\n",
    "                ) * l2_norm_params\n",
    "\n",
    "            dW, dB = self.backward_prop(y_batch)\n",
    "\n",
    "            # Weight decay in gradients\n",
    "            for i in range(self.layer_n - 1):\n",
    "                dW[i] += self.weight_decay * self.weights[i]\n",
    "\n",
    "            # Undo look-ahead\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] += self.lr * self.momentum * self.v_w[i]\n",
    "                self.biases[i] += self.lr * self.momentum * self.v_b[i]\n",
    "\n",
    "            # Update velocity\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.v_w[i] = self.momentum * self.v_w[i] + dW[i]\n",
    "                self.v_b[i] = self.momentum * self.v_b[i] + dB[i]\n",
    "\n",
    "            # Final param update\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.lr * self.v_w[i]\n",
    "                self.biases[i] -= self.lr * self.v_b[i]\n",
    "\n",
    "            return loss\n",
    "        else:\n",
    "            # Normal forward/back\n",
    "            self.forward_pass(x_batch)\n",
    "            out = self.h[-1]\n",
    "\n",
    "            # L2 norm\n",
    "            l2_norm_weights = sum(np.sum(w**2) for w in self.weights)\n",
    "            l2_norm_params = l2_norm_weights\n",
    "\n",
    "            # Loss\n",
    "            if self.loss == \"cross_entropy\":\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis=1)) + (\n",
    "                    self.weight_decay / 2\n",
    "                ) * l2_norm_params\n",
    "            elif self.loss == \"mean_squared_error\":\n",
    "                loss = 0.5 * np.mean(np.sum((out - y_batch) ** 2, axis=1))\n",
    "            else:\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis=1)) + (\n",
    "                    self.weight_decay / 2\n",
    "                ) * l2_norm_params\n",
    "\n",
    "            dW, dB = self.backward_prop(y_batch)\n",
    "            self._update_params(dW, dB)\n",
    "            return loss\n",
    "\n",
    "    # --- Outer Training Loop ---\n",
    "    def training(self, x_train, y_train, x_val, y_val, epochs, batch_size):\n",
    "        for ep in range(epochs):\n",
    "            idx = np.random.permutation(x_train.shape[0])\n",
    "            x_train_shuff = x_train[idx]\n",
    "            y_train_shuff = y_train[idx]\n",
    "            n_batches = len(x_train) // batch_size\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            # Batches\n",
    "            for b in range(n_batches):\n",
    "                start = b * batch_size\n",
    "                end = start + batch_size\n",
    "                x_batch = x_train_shuff[start:end]\n",
    "                y_batch = y_train_shuff[start:end]\n",
    "                loss = self._train_step(x_batch, y_batch)\n",
    "                epoch_loss += loss\n",
    "\n",
    "            avg_loss = epoch_loss / n_batches\n",
    "\n",
    "            # Validation\n",
    "            preds = self.predict(x_val)\n",
    "            val_labels = np.argmax(y_val, axis=1)\n",
    "            val_acc = np.mean(preds == val_labels)\n",
    "\n",
    "            val_outputs = self.forward_pass(x_val)[-1]\n",
    "\n",
    "            # Validation loss\n",
    "            l2_norm_weights = sum(np.sum(w**2) for w in self.weights)\n",
    "            l2_norm_params = l2_norm_weights\n",
    "\n",
    "            if self.loss == \"cross_entropy\":\n",
    "                val_loss = -np.mean(\n",
    "                    np.sum(y_val * np.log(val_outputs + 1e-10), axis=1)\n",
    "                ) + (self.weight_decay / 2) * l2_norm_params\n",
    "            elif self.loss == \"mean_squared_error\":\n",
    "                val_loss = 0.5 * np.mean(np.sum((val_outputs - y_val) ** 2, axis=1))\n",
    "            else:\n",
    "                val_loss = -np.mean(\n",
    "                    np.sum(y_val * np.log(val_outputs + 1e-10), axis=1)\n",
    "                ) + (self.weight_decay / 2) * l2_norm_params\n",
    "\n",
    "            # Log to wandb\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"epoch\": ep + 1,\n",
    "                    \"training_loss\": avg_loss,\n",
    "                    \"validation_accuracy\": val_acc,\n",
    "                    \"validation loss\": val_loss,\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Epoch {ep+1}/{epochs} - loss={avg_loss:.4f}, \"\n",
    "                f\"val_acc={val_acc:.4f}, val_loss={val_loss}\"\n",
    "            )\n",
    "\n",
    "    # --- Prediction ---\n",
    "    def predict(self, X):\n",
    "        self.forward_pass(X)\n",
    "        return np.argmax(self.h[-1], axis=1)\n",
    "\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train Neural Network with backpropagation.\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"-wp\",\n",
    "        \"--wandb_project\",\n",
    "        type=str,\n",
    "        default=\"myprojectname\",\n",
    "        help=\"Project name used to track experiments in Weights & Biases dashboard\",\n",
    "    )\n",
    "\n",
    "    # You can omit --wandb_entity or set it explicitly if you have permission:\n",
    "    # parser.add_argument('-we', '--wandb_entity', type=str, default='myusername', help='Wandb Entity')\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"-d\",\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        default=\"fashion_mnist\",\n",
    "        choices=[\"mnist\", \"fashion_mnist\"],\n",
    "        help=\"Dataset to use\",\n",
    "    )\n",
    "    parser.add_argument(\"-e\", \"--epochs\", type=int, default=1, help=\"Number of epochs to train neural network\")\n",
    "    parser.add_argument(\"-b\", \"--batch_size\", type=int, default=4, help=\"Batch size used to train neural network\")\n",
    "    parser.add_argument(\n",
    "        \"-l\",\n",
    "        \"--loss\",\n",
    "        type=str,\n",
    "        default=\"cross_entropy\",\n",
    "        choices=[\"mean_squared_error\", \"cross_entropy\"],\n",
    "        help=\"Loss function\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-o\",\n",
    "        \"--optimizer\",\n",
    "        type=str,\n",
    "        default=\"sgd\",\n",
    "        choices=[\"sgd\", \"momentum\", \"nag\", \"rmsprop\", \"adam\", \"nadam\"],\n",
    "        help=\"Optimizer type\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-lr\", \"--learning_rate\", type=float, default=0.1, help=\"Learning rate used to optimize model parameters\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-m\",\n",
    "        \"--momentum\",\n",
    "        type=float,\n",
    "        default=0.5,\n",
    "        help=\"Momentum used by momentum and nag optimizers\",\n",
    "    )\n",
    "    parser.add_argument(\"-beta\", \"--beta\", type=float, default=0.5, help=\"Beta used by rmsprop optimizer\")\n",
    "    parser.add_argument(\n",
    "        \"-beta1\",\n",
    "        \"--beta1\",\n",
    "        type=float,\n",
    "        default=0.5,\n",
    "        help=\"Beta1 used by adam and nadam optimizers\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-beta2\",\n",
    "        \"--beta2\",\n",
    "        type=float,\n",
    "        default=0.5,\n",
    "        help=\"Beta2 used by adam and nadam optimizers\",\n",
    "    )\n",
    "    parser.add_argument(\"-eps\", \"--epsilon\", type=float, default=1e-6, help=\"Epsilon used by optimizers\")\n",
    "    parser.add_argument(\n",
    "        \"-w_d\",\n",
    "        \"--weight_decay\",\n",
    "        type=float,\n",
    "        default=0.0,\n",
    "        help=\"Weight decay used by optimizers\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-w_i\",\n",
    "        \"--weight_init\",\n",
    "        type=str,\n",
    "        default=\"random\",\n",
    "        choices=[\"random\", \"Xavier\"],\n",
    "        help=\"Weight initialization method\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-nhl\",\n",
    "        \"--num_layers\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of hidden layers used in feedforward neural network\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-sz\",\n",
    "        \"--hidden_size\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help=\"Number of hidden neurons in each hidden layer\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-a\",\n",
    "        \"--activation\",\n",
    "        type=str,\n",
    "        default=\"sigmoid\",\n",
    "        choices=[\"identity\", \"sigmoid\", \"tanh\", \"ReLU\"],\n",
    "        help=\"Activation function\",\n",
    "    )\n",
    "\n",
    "    # Use parse_known_args to avoid errors with Jupyters extra flags\n",
    "    args, unknown = parser.parse_known_args()\n",
    "    config = vars(args)\n",
    "\n",
    "    # If you are logged in to wandb with the correct credentials,\n",
    "    # you can simply do:\n",
    "    # wandb.init(project=config['wandb_project'], config=config)\n",
    "\n",
    "    # If you get 403 permission denied or want a quick fix, use anonymous mode:\n",
    "    wandb.init(project=config[\"wandb_project\"], config=config, anonymous=\"allow\")\n",
    "\n",
    "    # Load data\n",
    "    if config[\"dataset\"] == \"fashion_mnist\":\n",
    "        (x_train_full, y_train_full), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    else:  # \"mnist\"\n",
    "        (x_train_full, y_train_full), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    x_train_full = x_train_full.reshape(x_train_full.shape[0], -1) / 255.0\n",
    "    x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n",
    "\n",
    "    np.random.seed(42)\n",
    "    idx = np.arange(x_train_full.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    x_train_full = x_train_full[idx]\n",
    "    y_train_full = y_train_full[idx]\n",
    "\n",
    "    # 90% train, 10% validation\n",
    "    train_size = int(0.9 * len(x_train_full))\n",
    "    x_train, y_train = x_train_full[:train_size], y_train_full[:train_size]\n",
    "    x_val, y_val = x_train_full[train_size:], y_train_full[train_size:]\n",
    "\n",
    "    num_classes = 10\n",
    "    y_train_1h = np.eye(num_classes)[y_train]\n",
    "    y_val_1h = np.eye(num_classes)[y_val]\n",
    "    y_test_1h = np.eye(num_classes)[y_test]\n",
    "\n",
    "    # Build model\n",
    "    # layers = [784] + [hidden_size] * num_layers + [10]\n",
    "    model = feed_forward_NN_final(\n",
    "        layers=[784] + [config[\"hidden_size\"]] * config[\"num_layers\"] + [10],\n",
    "        optimizer=config[\"optimizer\"],\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        momentum=config[\"momentum\"],\n",
    "        beta1=config[\"beta1\"],\n",
    "        beta2=config[\"beta2\"],\n",
    "        beta=config[\"beta\"],\n",
    "        epsilon=config[\"epsilon\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "        weight_init=config[\"weight_init\"],\n",
    "        activation=config[\"activation\"],\n",
    "        loss=config[\"loss\"],\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    model.training(\n",
    "        x_train=x_train,\n",
    "        y_train=y_train_1h,\n",
    "        x_val=x_val,\n",
    "        y_val=y_val_1h,\n",
    "        epochs=config[\"epochs\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "    )\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_preds = model.predict(x_test)\n",
    "    test_labels = np.argmax(y_test_1h, axis=1)\n",
    "    test_acc = np.mean(test_preds == test_labels)\n",
    "\n",
    "    wandb.log({\"test_accuracy\": test_acc})\n",
    "    print(\"Test accuracy:\", test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
