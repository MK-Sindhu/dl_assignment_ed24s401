{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33med24s401\u001b[0m (\u001b[33med24s401-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\acer\\OneDrive\\Documents\\GitHub\\dl_assignment_ed24s401\\wandb\\run-20250318_125316-pkdto9z3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ed24s401-indian-institute-of-technology-madras/myprojectname/runs/pkdto9z3?apiKey=f0880f1a8675dc5a9ff218689c5340669690b6e0' target=\"_blank\">clear-shape-6</a></strong> to <a href='https://wandb.ai/ed24s401-indian-institute-of-technology-madras/myprojectname?apiKey=f0880f1a8675dc5a9ff218689c5340669690b6e0' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ed24s401-indian-institute-of-technology-madras/myprojectname?apiKey=f0880f1a8675dc5a9ff218689c5340669690b6e0' target=\"_blank\">https://wandb.ai/ed24s401-indian-institute-of-technology-madras/myprojectname?apiKey=f0880f1a8675dc5a9ff218689c5340669690b6e0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ed24s401-indian-institute-of-technology-madras/myprojectname/runs/pkdto9z3?apiKey=f0880f1a8675dc5a9ff218689c5340669690b6e0' target=\"_blank\">https://wandb.ai/ed24s401-indian-institute-of-technology-madras/myprojectname/runs/pkdto9z3?apiKey=f0880f1a8675dc5a9ff218689c5340669690b6e0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Do NOT share these links with anyone. They can be used to claim your runs."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1 - loss=0.7907, val_acc=0.6962, val_loss=0.8374731050382146\n",
      "Test accuracy: 0.6956\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.datasets import fashion_mnist\n",
    "import wandb\n",
    "import argparse\n",
    "\n",
    "\n",
    "# Neural Network Class: feed_forward_NN_final\n",
    "class feed_forward_NN_final:\n",
    "    def __init__(\n",
    "        self,\n",
    "        layers,\n",
    "        optimizer,\n",
    "        learning_rate,\n",
    "        momentum,\n",
    "        beta1,\n",
    "        beta2,\n",
    "        beta,\n",
    "        epsilon,\n",
    "        weight_decay,\n",
    "        weight_init,\n",
    "        activation,\n",
    "        loss\n",
    "    ):\n",
    "        self.layers = layers\n",
    "        self.layer_n = len(layers)\n",
    "        self.optimizer = optimizer.lower()\n",
    "        self.lr = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.beta_rms = beta\n",
    "        self.epsilon = epsilon\n",
    "        self.weight_decay = weight_decay\n",
    "        self.weight_init = weight_init.lower()\n",
    "        self.activation = activation.lower()\n",
    "        self.loss = loss.lower()\n",
    "\n",
    "        # Initialize Weights & Biases\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        for i in range(self.layer_n - 1):\n",
    "            if self.weight_init == \"xavier\":\n",
    "                # \"Xavier\" initialization\n",
    "                w = np.random.randn(layers[i], layers[i + 1]) * np.sqrt(1.0 / layers[i])\n",
    "            else:\n",
    "                # \"random\" initialization\n",
    "                w = np.random.randn(layers[i], layers[i + 1]) * np.sqrt(2.0 / layers[i])\n",
    "            b = np.zeros((1, layers[i + 1]))\n",
    "            self.weights.append(w)\n",
    "            self.biases.append(b)\n",
    "\n",
    "        # Initialize extra params if needed\n",
    "        if self.optimizer in [\"momentum\", \"nag\", \"rmsprop\", \"adam\", \"nadam\"]:\n",
    "            self.v_w = [np.zeros_like(w) for w in self.weights]\n",
    "            self.v_b = [np.zeros_like(b) for b in self.biases]\n",
    "        if self.optimizer in [\"adam\", \"nadam\"]:\n",
    "            self.m_w = [np.zeros_like(w) for w in self.weights]\n",
    "            self.m_b = [np.zeros_like(b) for b in self.biases]\n",
    "            self.t = 0\n",
    "\n",
    "    # --- Activation Functions ---\n",
    "    def sigmoid(self, x):\n",
    "        # Clip to avoid overflow in exp()\n",
    "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def activate(self, x):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return self.sigmoid(x)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return self.tanh(x)\n",
    "        elif self.activation == \"relu\":\n",
    "            return self.relu(x)\n",
    "        else:\n",
    "            # default\n",
    "            return self.sigmoid(x)\n",
    "\n",
    "    # Derivatives of Activation\n",
    "    def derivative(self, a):\n",
    "        if self.activation == \"sigmoid\":\n",
    "            return a * (1 - a)\n",
    "        elif self.activation == \"tanh\":\n",
    "            return 1 - a**2\n",
    "        elif self.activation == \"relu\":\n",
    "            return (a > 0).astype(float)\n",
    "        else:\n",
    "            return a * (1 - a)\n",
    "\n",
    "    def softmax(self, x):\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "\n",
    "    # --- Forward Pass ---\n",
    "    def forward_pass(self, x):\n",
    "        self.h = [x]\n",
    "        # Hidden layers\n",
    "        for i in range(self.layer_n - 2):\n",
    "            z = np.dot(self.h[i], self.weights[i]) + self.biases[i]\n",
    "            act = self.activate(z)\n",
    "            self.h.append(act)\n",
    "        # Output layer: softmax\n",
    "        z_out = np.dot(self.h[-1], self.weights[-1]) + self.biases[-1]\n",
    "        out = self.softmax(z_out)\n",
    "        self.h.append(out)\n",
    "        return self.h\n",
    "\n",
    "    # --- Backward Pass ---\n",
    "    def backward_prop(self, y_true):\n",
    "        m = y_true.shape[0]\n",
    "        dw = [None] * (self.layer_n - 1)\n",
    "        db = [None] * (self.layer_n - 1)\n",
    "\n",
    "        # Output layer delta\n",
    "        if self.loss == \"cross_entropy\":\n",
    "            # Cross-entropy derivative\n",
    "            delta = self.h[-1] - y_true\n",
    "        elif self.loss == \"mean_squared_error\":\n",
    "            # MSE derivative wrt softmax\n",
    "            batch_size_sq = len(self.h[-1])\n",
    "            classes_sq = len(self.h[-1][0])\n",
    "            delta = np.zeros((batch_size_sq, classes_sq))\n",
    "            for i in range(batch_size_sq):\n",
    "                jacobian_softmax = (\n",
    "                    np.diag(self.h[-1][i]) - np.outer(self.h[-1][i], self.h[-1][i])\n",
    "                )\n",
    "                delta[i] = 2 * np.dot(self.h[-1][i] - y_true[i], jacobian_softmax)\n",
    "        else:\n",
    "            # default to cross-entropy style\n",
    "            delta = self.h[-1] - y_true\n",
    "\n",
    "        # Backprop through layers\n",
    "        for i in reversed(range(self.layer_n - 1)):\n",
    "            dw[i] = np.dot(self.h[i].T, delta) / m\n",
    "            db[i] = np.sum(delta, axis=0, keepdims=True) / m\n",
    "            if i > 0:\n",
    "                delta = np.dot(delta, self.weights[i].T) * self.derivative(self.h[i])\n",
    "        return dw, db\n",
    "\n",
    "    # --- Parameter Updates (Non-Nesterov) ---\n",
    "    def _update_params(self, dw, db):\n",
    "        # Add weight decay\n",
    "        for i in range(self.layer_n - 1):\n",
    "            dw[i] += self.weight_decay * self.weights[i]\n",
    "\n",
    "        if self.optimizer == \"sgd\":\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.lr * dw[i]\n",
    "                self.biases[i] -= self.lr * db[i]\n",
    "\n",
    "        elif self.optimizer == \"momentum\":\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.v_w[i] = self.momentum * self.v_w[i] + dw[i]\n",
    "                self.v_b[i] = self.momentum * self.v_b[i] + db[i]\n",
    "                self.weights[i] -= self.lr * self.v_w[i]\n",
    "                self.biases[i] -= self.lr * self.v_b[i]\n",
    "\n",
    "        elif self.optimizer == \"rmsprop\":\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.v_w[i] = self.beta_rms * self.v_w[i] + (1 - self.beta_rms) * (\n",
    "                    dw[i] ** 2\n",
    "                )\n",
    "                self.v_b[i] = self.beta_rms * self.v_b[i] + (1 - self.beta_rms) * (\n",
    "                    db[i] ** 2\n",
    "                )\n",
    "                self.weights[i] -= self.lr * dw[i] / (np.sqrt(self.v_w[i]) + self.epsilon)\n",
    "                self.biases[i] -= self.lr * db[i] / (np.sqrt(self.v_b[i]) + self.epsilon)\n",
    "\n",
    "        elif self.optimizer == \"adam\":\n",
    "            self.t += 1\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * dw[i]\n",
    "                self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * db[i]\n",
    "                self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (dw[i] ** 2)\n",
    "                self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (db[i] ** 2)\n",
    "\n",
    "                # Bias correction\n",
    "                m_w_hat = self.m_w[i] / (1 - self.beta1**self.t)\n",
    "                m_b_hat = self.m_b[i] / (1 - self.beta1**self.t)\n",
    "                v_w_hat = self.v_w[i] / (1 - self.beta2**self.t)\n",
    "                v_b_hat = self.v_b[i] / (1 - self.beta2**self.t)\n",
    "\n",
    "                self.weights[i] -= self.lr * m_w_hat / (np.sqrt(v_w_hat) + self.epsilon)\n",
    "                self.biases[i] -= self.lr * m_b_hat / (np.sqrt(v_b_hat) + self.epsilon)\n",
    "\n",
    "        elif self.optimizer == \"nadam\":\n",
    "            self.t += 1\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.m_w[i] = self.beta1 * self.m_w[i] + (1 - self.beta1) * dw[i]\n",
    "                self.m_b[i] = self.beta1 * self.m_b[i] + (1 - self.beta1) * db[i]\n",
    "                self.v_w[i] = self.beta2 * self.v_w[i] + (1 - self.beta2) * (dw[i] ** 2)\n",
    "                self.v_b[i] = self.beta2 * self.v_b[i] + (1 - self.beta2) * (db[i] ** 2)\n",
    "\n",
    "                # Bias correction\n",
    "                m_w_hat = self.m_w[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "                m_b_hat = self.m_b[i] / (1 - self.beta1 ** (self.t + 1))\n",
    "                v_w_hat = self.v_w[i] / (1 - self.beta2 ** (self.t + 1))\n",
    "                v_b_hat = self.v_b[i] / (1 - self.beta2 ** (self.t + 1))\n",
    "\n",
    "                grad_term_w = self.beta1 * m_w_hat + (1 - self.beta1) * dw[i] / (\n",
    "                    1 - self.beta1 ** (self.t + 1)\n",
    "                )\n",
    "                grad_term_b = self.beta1 * m_b_hat + (1 - self.beta1) * db[i] / (\n",
    "                    1 - self.beta1 ** (self.t + 1)\n",
    "                )\n",
    "\n",
    "                self.weights[i] -= self.lr * grad_term_w / (\n",
    "                    np.sqrt(v_w_hat) + self.epsilon\n",
    "                )\n",
    "                self.biases[i] -= self.lr * grad_term_b / (\n",
    "                    np.sqrt(v_b_hat) + self.epsilon\n",
    "                )\n",
    "\n",
    "    # --- Training Step (Includes Nesterov) ---\n",
    "    def _train_step(self, x_batch, y_batch):\n",
    "        if self.optimizer == \"nag\":\n",
    "            # Look-ahead\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.lr * self.momentum * self.v_w[i]\n",
    "                self.biases[i] -= self.lr * self.momentum * self.v_b[i]\n",
    "\n",
    "            self.forward_pass(x_batch)\n",
    "            out = self.h[-1]\n",
    "\n",
    "            # L2 norm (for weight decay)\n",
    "            l2_norm_weights = sum(np.sum(w**2) for w in self.weights)\n",
    "            l2_norm_params = l2_norm_weights\n",
    "\n",
    "            # Loss\n",
    "            if self.loss == \"cross_entropy\":\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis=1)) + (\n",
    "                    self.weight_decay / 2\n",
    "                ) * l2_norm_params\n",
    "            elif self.loss == \"mean_squared_error\":\n",
    "                loss = 0.5 * np.mean(np.sum((out - y_batch) ** 2, axis=1))\n",
    "            else:\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis=1)) + (\n",
    "                    self.weight_decay / 2\n",
    "                ) * l2_norm_params\n",
    "\n",
    "            dW, dB = self.backward_prop(y_batch)\n",
    "\n",
    "            # Weight decay in gradients\n",
    "            for i in range(self.layer_n - 1):\n",
    "                dW[i] += self.weight_decay * self.weights[i]\n",
    "\n",
    "            # Undo look-ahead\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] += self.lr * self.momentum * self.v_w[i]\n",
    "                self.biases[i] += self.lr * self.momentum * self.v_b[i]\n",
    "\n",
    "            # Update velocity\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.v_w[i] = self.momentum * self.v_w[i] + dW[i]\n",
    "                self.v_b[i] = self.momentum * self.v_b[i] + dB[i]\n",
    "\n",
    "            # Final param update\n",
    "            for i in range(self.layer_n - 1):\n",
    "                self.weights[i] -= self.lr * self.v_w[i]\n",
    "                self.biases[i] -= self.lr * self.v_b[i]\n",
    "\n",
    "            return loss\n",
    "        else:\n",
    "            # Normal forward/back\n",
    "            self.forward_pass(x_batch)\n",
    "            out = self.h[-1]\n",
    "\n",
    "            # L2 norm\n",
    "            l2_norm_weights = sum(np.sum(w**2) for w in self.weights)\n",
    "            l2_norm_params = l2_norm_weights\n",
    "\n",
    "            # Loss\n",
    "            if self.loss == \"cross_entropy\":\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis=1)) + (\n",
    "                    self.weight_decay / 2\n",
    "                ) * l2_norm_params\n",
    "            elif self.loss == \"mean_squared_error\":\n",
    "                loss = 0.5 * np.mean(np.sum((out - y_batch) ** 2, axis=1))\n",
    "            else:\n",
    "                loss = -np.mean(np.sum(y_batch * np.log(out + 1e-10), axis=1)) + (\n",
    "                    self.weight_decay / 2\n",
    "                ) * l2_norm_params\n",
    "\n",
    "            dW, dB = self.backward_prop(y_batch)\n",
    "            self._update_params(dW, dB)\n",
    "            return loss\n",
    "\n",
    "    # --- Outer Training Loop ---\n",
    "    def training(self, x_train, y_train, x_val, y_val, epochs, batch_size):\n",
    "        for ep in range(epochs):\n",
    "            idx = np.random.permutation(x_train.shape[0])\n",
    "            x_train_shuff = x_train[idx]\n",
    "            y_train_shuff = y_train[idx]\n",
    "            n_batches = len(x_train) // batch_size\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            # Batches\n",
    "            for b in range(n_batches):\n",
    "                start = b * batch_size\n",
    "                end = start + batch_size\n",
    "                x_batch = x_train_shuff[start:end]\n",
    "                y_batch = y_train_shuff[start:end]\n",
    "                loss = self._train_step(x_batch, y_batch)\n",
    "                epoch_loss += loss\n",
    "\n",
    "            avg_loss = epoch_loss / n_batches\n",
    "\n",
    "            # Validation\n",
    "            preds = self.predict(x_val)\n",
    "            val_labels = np.argmax(y_val, axis=1)\n",
    "            val_acc = np.mean(preds == val_labels)\n",
    "\n",
    "            val_outputs = self.forward_pass(x_val)[-1]\n",
    "\n",
    "            # Validation loss\n",
    "            l2_norm_weights = sum(np.sum(w**2) for w in self.weights)\n",
    "            l2_norm_params = l2_norm_weights\n",
    "\n",
    "            if self.loss == \"cross_entropy\":\n",
    "                val_loss = -np.mean(\n",
    "                    np.sum(y_val * np.log(val_outputs + 1e-10), axis=1)\n",
    "                ) + (self.weight_decay / 2) * l2_norm_params\n",
    "            elif self.loss == \"mean_squared_error\":\n",
    "                val_loss = 0.5 * np.mean(np.sum((val_outputs - y_val) ** 2, axis=1))\n",
    "            else:\n",
    "                val_loss = -np.mean(\n",
    "                    np.sum(y_val * np.log(val_outputs + 1e-10), axis=1)\n",
    "                ) + (self.weight_decay / 2) * l2_norm_params\n",
    "\n",
    "            # Log to wandb\n",
    "            wandb.log(\n",
    "                {\n",
    "                    \"epoch\": ep + 1,\n",
    "                    \"training_loss\": avg_loss,\n",
    "                    \"validation_accuracy\": val_acc,\n",
    "                    \"validation loss\": val_loss,\n",
    "                }\n",
    "            )\n",
    "            print(\n",
    "                f\"Epoch {ep+1}/{epochs} - loss={avg_loss:.4f}, \"\n",
    "                f\"val_acc={val_acc:.4f}, val_loss={val_loss}\"\n",
    "            )\n",
    "\n",
    "    # --- Prediction ---\n",
    "    def predict(self, X):\n",
    "        self.forward_pass(X)\n",
    "        return np.argmax(self.h[-1], axis=1)\n",
    "\n",
    "\n",
    "# --- Main execution block ---\n",
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Train Neural Network with backpropagation.\")\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"-wp\",\n",
    "        \"--wandb_project\",\n",
    "        type=str,\n",
    "        default=\"myprojectname\",\n",
    "        help=\"Project name used to track experiments in Weights & Biases dashboard\",\n",
    "    )\n",
    "\n",
    "    # You can omit --wandb_entity or set it explicitly if you have permission:\n",
    "    # parser.add_argument('-we', '--wandb_entity', type=str, default='myusername', help='Wandb Entity')\n",
    "\n",
    "    parser.add_argument(\n",
    "        \"-d\",\n",
    "        \"--dataset\",\n",
    "        type=str,\n",
    "        default=\"fashion_mnist\",\n",
    "        choices=[\"mnist\", \"fashion_mnist\"],\n",
    "        help=\"Dataset to use\",\n",
    "    )\n",
    "    parser.add_argument(\"-e\", \"--epochs\", type=int, default=1, help=\"Number of epochs to train neural network\")\n",
    "    parser.add_argument(\"-b\", \"--batch_size\", type=int, default=4, help=\"Batch size used to train neural network\")\n",
    "    parser.add_argument(\n",
    "        \"-l\",\n",
    "        \"--loss\",\n",
    "        type=str,\n",
    "        default=\"cross_entropy\",\n",
    "        choices=[\"mean_squared_error\", \"cross_entropy\"],\n",
    "        help=\"Loss function\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-o\",\n",
    "        \"--optimizer\",\n",
    "        type=str,\n",
    "        default=\"sgd\",\n",
    "        choices=[\"sgd\", \"momentum\", \"nag\", \"rmsprop\", \"adam\", \"nadam\"],\n",
    "        help=\"Optimizer type\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-lr\", \"--learning_rate\", type=float, default=0.1, help=\"Learning rate used to optimize model parameters\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-m\",\n",
    "        \"--momentum\",\n",
    "        type=float,\n",
    "        default=0.5,\n",
    "        help=\"Momentum used by momentum and nag optimizers\",\n",
    "    )\n",
    "    parser.add_argument(\"-beta\", \"--beta\", type=float, default=0.5, help=\"Beta used by rmsprop optimizer\")\n",
    "    parser.add_argument(\n",
    "        \"-beta1\",\n",
    "        \"--beta1\",\n",
    "        type=float,\n",
    "        default=0.5,\n",
    "        help=\"Beta1 used by adam and nadam optimizers\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-beta2\",\n",
    "        \"--beta2\",\n",
    "        type=float,\n",
    "        default=0.5,\n",
    "        help=\"Beta2 used by adam and nadam optimizers\",\n",
    "    )\n",
    "    parser.add_argument(\"-eps\", \"--epsilon\", type=float, default=1e-6, help=\"Epsilon used by optimizers\")\n",
    "    parser.add_argument(\n",
    "        \"-w_d\",\n",
    "        \"--weight_decay\",\n",
    "        type=float,\n",
    "        default=0.0,\n",
    "        help=\"Weight decay used by optimizers\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-w_i\",\n",
    "        \"--weight_init\",\n",
    "        type=str,\n",
    "        default=\"random\",\n",
    "        choices=[\"random\", \"Xavier\"],\n",
    "        help=\"Weight initialization method\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-nhl\",\n",
    "        \"--num_layers\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of hidden layers used in feedforward neural network\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-sz\",\n",
    "        \"--hidden_size\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help=\"Number of hidden neurons in each hidden layer\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"-a\",\n",
    "        \"--activation\",\n",
    "        type=str,\n",
    "        default=\"sigmoid\",\n",
    "        choices=[\"identity\", \"sigmoid\", \"tanh\", \"ReLU\"],\n",
    "        help=\"Activation function\",\n",
    "    )\n",
    "\n",
    "    \n",
    "    args, unknown = parser.parse_known_args()\n",
    "    config = vars(args)\n",
    "\n",
    "    wandb.init(project=config[\"wandb_project\"], config=config, anonymous=\"allow\")\n",
    "\n",
    "    # Load data\n",
    "    if config[\"dataset\"] == \"fashion_mnist\":\n",
    "        (x_train_full, y_train_full), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    else:  # \"mnist\"\n",
    "        (x_train_full, y_train_full), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "    x_train_full = x_train_full.reshape(x_train_full.shape[0], -1) / 255.0\n",
    "    x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n",
    "\n",
    "    np.random.seed(42)\n",
    "    idx = np.arange(x_train_full.shape[0])\n",
    "    np.random.shuffle(idx)\n",
    "    x_train_full = x_train_full[idx]\n",
    "    y_train_full = y_train_full[idx]\n",
    "\n",
    "    # 90% train, 10% validation\n",
    "    train_size = int(0.9 * len(x_train_full))\n",
    "    x_train, y_train = x_train_full[:train_size], y_train_full[:train_size]\n",
    "    x_val, y_val = x_train_full[train_size:], y_train_full[train_size:]\n",
    "\n",
    "    num_classes = 10\n",
    "    y_train_1h = np.eye(num_classes)[y_train]\n",
    "    y_val_1h = np.eye(num_classes)[y_val]\n",
    "    y_test_1h = np.eye(num_classes)[y_test]\n",
    "\n",
    "    # Build model\n",
    "    # layers = [784] + [hidden_size] * num_layers + [10]\n",
    "    model = feed_forward_NN_final(\n",
    "        layers=[784] + [config[\"hidden_size\"]] * config[\"num_layers\"] + [10],\n",
    "        optimizer=config[\"optimizer\"],\n",
    "        learning_rate=config[\"learning_rate\"],\n",
    "        momentum=config[\"momentum\"],\n",
    "        beta1=config[\"beta1\"],\n",
    "        beta2=config[\"beta2\"],\n",
    "        beta=config[\"beta\"],\n",
    "        epsilon=config[\"epsilon\"],\n",
    "        weight_decay=config[\"weight_decay\"],\n",
    "        weight_init=config[\"weight_init\"],\n",
    "        activation=config[\"activation\"],\n",
    "        loss=config[\"loss\"],\n",
    "    )\n",
    "\n",
    "    # Train\n",
    "    model.training(\n",
    "        x_train=x_train,\n",
    "        y_train=y_train_1h,\n",
    "        x_val=x_val,\n",
    "        y_val=y_val_1h,\n",
    "        epochs=config[\"epochs\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "    )\n",
    "\n",
    "    # Evaluate on test set\n",
    "    test_preds = model.predict(x_test)\n",
    "    test_labels = np.argmax(y_test_1h, axis=1)\n",
    "    test_acc = np.mean(test_preds == test_labels)\n",
    "\n",
    "    wandb.log({\"test_accuracy\": test_acc})\n",
    "    print(\"Test accuracy:\", test_acc)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
